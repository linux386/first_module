{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  make model and save model\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "df = pd.read_csv('dataset/sonar.csv', header=None)\n",
    "'''\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "'''\n",
    "dataset = df.values\n",
    "X = dataset[:,0:60]\n",
    "Y_obj = dataset[:,60]\n",
    "\n",
    "e = LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)\n",
    "# 학습셋과 테스트셋을 나눔\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
    "model.save('model/my_model.h5')  # 모델을 컴퓨터에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위해 메모리 내의 모델을 삭제 후 저장된 모델을 loading \n",
    "\n",
    "del model       \n",
    "model = load_model('model/my_model.h5') # 모델을 새로 불러옴\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))  # 불러온 모델로 테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  loding saved Model \n",
    " \n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "df = pd.read_csv('dataset/sonar.csv', header=None)\n",
    "'''\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "'''\n",
    "dataset = df.values\n",
    "X = dataset[:,0:60]\n",
    "Y_obj = dataset[:,60]\n",
    "\n",
    "e = LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)\n",
    "# 학습셋과 테스트셋을 나눔\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "model = load_model('model/my_model.h5') # 모델을 새로 불러옴\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))  # 불러온 모델로 테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 저장하고 재사용하는 방법을 익혀봅니다.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data = np.loadtxt('./data.csv', delimiter=',',\n",
    "                  unpack=True, dtype='float32')\n",
    "\n",
    "# 털, 날개, 기타, 포유류, 조류\n",
    "# x_data = 0, 1\n",
    "# y_data = 2, 3, 4\n",
    "x_data = np.transpose(data[0:2])\n",
    "y_data = np.transpose(data[2:])\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "# 학습에 직접적으로 사용하지 않고 학습 횟수에 따라 단순히 증가시킬 변수를 만듭니다.\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "\n",
    "W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.))\n",
    "model = tf.matmul(L2, W3)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "# global_step로 넘겨준 변수를, 학습용 변수들을 최적화 할 때 마다 학습 횟수를 하나씩 증가시킵니다.\n",
    "train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "# 모델을 저장하고 불러오는 API를 초기화합니다.\n",
    "# global_variables 함수를 통해 앞서 정의하였던 변수들을 저장하거나 불러올 변수들로 설정합니다.\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 최적화 진행\n",
    "for step in range(2):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print('Step: %d, ' % sess.run(global_step),\n",
    "          'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "# 최적화가 끝난 뒤, 변수를 저장합니다.\n",
    "saver.save(sess, './model/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서보드를 이용하기 위해 각종 변수들을 설정하고 저장하는 방법을 익혀봅니다.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data = np.loadtxt('./data.csv', delimiter=',',\n",
    "                  unpack=True, dtype='float32')\n",
    "\n",
    "x_data = np.transpose(data[0:2])\n",
    "y_data = np.transpose(data[2:])\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# with tf.name_scope 으로 묶은 블럭은 텐서보드에서 한 레이어안에 표현해줍니다\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.), name='W1')\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.), name='W2')\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.), name='W3')\n",
    "    model = tf.matmul(L2, W3)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    # tf.summary.scalar 를 이용해 수집하고 싶은 값들을 지정할 수 있습니다.\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 텐서보드에서 표시해주기 위한 텐서들을 수집합니다.\n",
    "merged = tf.summary.merge_all()\n",
    "# 저장할 그래프와 텐서값들을 저장할 디렉토리를 설정합니다.\n",
    "writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "# 이렇게 저장한 로그는, 학습 후 다음의 명령어를 이용해 웹서버를 실행시킨 뒤\n",
    "# tensorboard --logdir=./logs\n",
    "# 다음 주소와 웹브라우저를 이용해 텐서보드에서 확인할 수 있습니다.\n",
    "# http://localhost:6006\n",
    "\n",
    "# 최적화 진행\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print('Step: %d, ' % sess.run(global_step),\n",
    "          'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "    # 적절한 시점에 저장할 값들을 수집하고 저장합니다.\n",
    "    summary = sess.run(merged, feed_dict={X: x_data, Y: y_data})\n",
    "    writer.add_summary(summary, global_step=sess.run(global_step))\n",
    "\n",
    "saver.save(sess, './model/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simple save _1\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None], name=\"input\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"output\")\n",
    "\n",
    "hypothesis = X * W + b\n",
    "hypothesis = tf.identity(hypothesis, \"hypothesis\")\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val,_ = sess.run([cost,train],feed_dict={X: x_train, Y: y_train})\n",
    "    if(step % 100 == 0):\n",
    "        print(step,cost_val)\n",
    "\n",
    "saver.save(sess, './my_model/my_model', global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simple save _2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None], name=\"input\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"output\")\n",
    "\n",
    "hypothesis = X * W + b\n",
    "hypothesis = tf.identity(hypothesis, \"hypothesis\")\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: x_train, Y: y_train})\n",
    "    if(step % 100 == 0):\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "\n",
    "saver.save(sess, './my_model/my_model', global_step=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simple restore\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('my_model/my_model-1000.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./my_model'))\n",
    "\n",
    "tf.get_default_graph()\n",
    "\n",
    "x = sess.graph.get_tensor_by_name(\"input:0\")\n",
    "y = sess.graph.get_tensor_by_name(\"output:0\")\n",
    "hypothesis = sess.graph.get_tensor_by_name(\"hypothesis:0\")\n",
    "\n",
    "result = sess.run(hypothesis, feed_dict={x:[4,3,6]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simple restore _3\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('model/simple_model.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./model'))\n",
    "#new_saver.restore(sess, 'model/simple_model')\n",
    "\n",
    "tf.get_default_graph()\n",
    "\n",
    "x = sess.graph.get_tensor_by_name(\"input:0\")\n",
    "y = sess.graph.get_tensor_by_name(\"output:0\")\n",
    "hypothesis = sess.graph.get_tensor_by_name(\"hypothesis:0\")\n",
    "\n",
    "result = sess.run(hypothesis, feed_dict={x:[1, 2, 3]})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 학습의 Hello World 와 같은 MNIST(손글씨 숫자 인식) 문제를 신경망으로 풀어봅니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# 텐서플로우에 기본 내장된 mnist 모듈을 이용하여 데이터를 로드합니다.\n",
    "# 지정한 폴더에 MNIST 데이터가 없는 경우 자동으로 데이터를 다운로드합니다.\n",
    "# one_hot 옵션은 레이블을 동물 분류 예제에서 보았던 one_hot 방식의 데이터로 만들어줍니다.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./\", one_hot=True)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "# 입력 값의 차원은 [배치크기, 특성값] 으로 되어 있습니다.\n",
    "# 손글씨 이미지는 28x28 픽셀로 이루어져 있고, 이를 784개의 특성값으로 정합니다.\n",
    "X = tf.placeholder(tf.float32, [None, 784], name='mnist_input')\n",
    "# 결과는 0~9 의 10 가지 분류를 가집니다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10], name ='mnist_output')\n",
    "\n",
    "# 신경망의 레이어는 다음처럼 구성합니다.\n",
    "# 784(입력 특성값)\n",
    "#   -> 256 (히든레이어 뉴런 갯수) -> 256 (히든레이어 뉴런 갯수)\n",
    "#   -> 10 (결과값 0~9 분류)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
    "# 입력값에 가중치를 곱하고 ReLU 함수를 이용하여 레이어를 만듭니다.\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n",
    "# L1 레이어의 출력값에 가중치를 곱하고 ReLU 함수를 이용하여 레이어를 만듭니다.\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "# 최종 모델의 출력값은 W3 변수를 곱해 10개의 분류를 가지게 됩니다.\n",
    "model = tf.matmul(L2, W3)\n",
    "model = tf.identity(model, \"mnist_model\")\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        # 텐서플로우의 mnist 모델의 next_batch 함수를 이용해\n",
    "        # 지정한 크기만큼 학습할 데이터를 가져옵니다.\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료!')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, 'model/mnist/mnist_model.ckpt')\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "######\n",
    "# model 로 예측한 값과 실제 레이블인 Y의 값을 비교합니다.\n",
    "# tf.argmax 함수를 이용해 예측한 값에서 가장 큰 값을 예측한 레이블이라고 평가합니다.\n",
    "# 예) [0.1 0 0 0.7 0 0.2 0 0 0 0] -> 3\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy,feed_dict={X: mnist.test.images,Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('image.pkl','wb')\n",
    "pickle.dump(mnist.test.images,f)\n",
    "f.close()\n",
    "\n",
    "f = open('label.pkl','wb')\n",
    "pickle.dump(mnist.test.labels,f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('image.pkl','rb')\n",
    "image = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('label.pkl','rb')\n",
    "label = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "f = open('image.pkl', 'rb')\n",
    "image = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('label.pkl', 'rb')\n",
    "label = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('model/mnist/mnist_model.ckpt.meta')\n",
    "new_saver.restore(sess, 'model/mnist/mnist_model.ckpt')\n",
    "\n",
    "tf.get_default_graph()\n",
    "\n",
    "x = sess.graph.get_tensor_by_name(\"mnist_input:0\")\n",
    "y = sess.graph.get_tensor_by_name(\"mnist_output:0\")\n",
    "model = sess.graph.get_tensor_by_name(\"mnist_model:0\")\n",
    "\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy,feed_dict={x: image,y: label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script shows how to predict stock prices using a basic RNN\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"d:\\\\kkang.xlsx\")\n",
    "df = df.iloc[:20]\n",
    "xy = df.values\n",
    "print(df)\n",
    "#row = len(xy)/5\n",
    "#xy = xy.reshape(int(row),5)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "#xy = np.loadtxt('d:\\\\kkang.csv', delimiter=',')\n",
    "#xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "pre_train_set = xy[0:train_size]\n",
    "pre_test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "print(\"pre_train_set:{}\".format(pre_train_set))\n",
    "print(\"pre_test_set:{}\".format(pre_test_set))\n",
    "\n",
    "# Scale each\n",
    "train_set = MinMaxScaler(pre_train_set)\n",
    "test_set = MinMaxScaler(pre_test_set)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script shows how to predict stock prices using a basic RNN\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"d:\\\\kkang.xlsx\")\n",
    "df = df.iloc[:20]\n",
    "xy = df.values\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "pre_train_set = xy[0:train_size]\n",
    "pre_test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :4]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainx, trainy = build_dataset(pre_train_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "testx, testy = build_dataset(pre_test_set, seq_length)\n",
    "\n",
    "\n",
    "# Scale each\n",
    "trainX = MinMaxScaler(trainx)\n",
    "trainY = MinMaxScaler(trainy)\n",
    "testX = MinMaxScaler(testx)\n",
    "testY = MinMaxScaler(testy)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Stock Prediction with LSTM_first\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "f = open('./stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "    \n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# Choose stock\n",
    "stock = \"010040\"\n",
    "pred_label = 4 # 0 : Open, 1: High 2: Low, 3:Volume 4:Close\n",
    "\n",
    "# 날짜 지정\n",
    "now=datetime.now()\n",
    "year = now.year\n",
    "month = now.month\n",
    "day = now.day\n",
    "\n",
    "start = datetime(2018, 1, 2)\n",
    "end = datetime(year, month, day)\n",
    "df_stock = fdr.DataReader(stock, start, end)\n",
    "\n",
    "df = df_stock[df_stock.Volume > 0]\n",
    "df = df[['Open','High','Low','Volume','Close']]\n",
    "\n",
    "pred_label = 4 # 0 : Open, 1: High 2: Low, 3:Volume 4:Close\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "print(\"seq_length:{}\\t data_dim:{}\\t hidden_dim:{}\\t learning_rate:{}\\t iterations:{}\"\\\n",
    "      .format(seq_length,data_dim,hidden_dim,learning_rate,iterations), file = f)\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = df.values\n",
    "#xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_set = xy[0:train_size]\n",
    "test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "last_set = xy[-seq_length:,:]\n",
    "          \n",
    "# Open, High, Low, Volume, Close\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(xy)\n",
    "\n",
    "#최종 7 rows 다음날 Price Predict 입력 data\n",
    "last_X = (xy[-seq_length:,:]-test_min)/(test_denom+1e-7);\n",
    "\n",
    "# Scale each\n",
    "train_set = MinMaxScaler(train_set)\n",
    "test_set = MinMaxScaler(test_set)\n",
    "#last_set = MinMaxScaler(last_set)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "lastX, lastY = build_dataset(last_set, seq_length)            \n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    \n",
    "    \n",
    "    # Predictions test\n",
    "    prediction_last = sess.run(Y_pred, feed_dict={X: last_X.reshape(1, 7, 5)})\n",
    "\n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "print(prediction_last *test_denom[pred_label]+test_min[pred_label])    \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Stock Prediction with LSTM_Second\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "f = open('stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_second =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "df = pd.read_csv('d:\\\\telcon.csv',header=None)\n",
    "#df = df.iloc[:100]\n",
    "xy = df.values\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "print(\"seq_length:{}\\t data_dim:{}\\t hidden_dim:{}\\t learning_rate:{}\\t iterations:{}\"\\\n",
    "      .format(seq_length,data_dim,hidden_dim,learning_rate,iterations), file = f)\n",
    "\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "#xy = np.loadtxt('d:\\\\kkang.csv', delimiter=',')\n",
    "#xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "pre_train_set = xy[0:train_size]\n",
    "pre_test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(xy)\n",
    "\n",
    "\n",
    "#최종 7 rows 다음날 Price Predict 입력 data\n",
    "last_X = (xy[-seq_length:,:]-test_min)/(test_denom+1e-7);\n",
    "\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainx, trainy = build_dataset(pre_train_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "testx, testy = build_dataset(pre_test_set, seq_length)\n",
    "\n",
    "# Scale each\n",
    "trainX = MinMaxScaler(trainx)\n",
    "trainY = MinMaxScaler(trainy)\n",
    "testX = MinMaxScaler(testx)\n",
    "testY = MinMaxScaler(testy)\n",
    "\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "\n",
    "    \n",
    "    # Predictions test\n",
    "    prediction_last = sess.run(Y_pred, feed_dict={X: last_X.reshape(1, 7, 5)})\n",
    "\n",
    "    \n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "print(prediction_last *test_denom[pred_label]+test_min[pred_label])    \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Stock Prediction with LSTM_Third\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "f = open('stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_third =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "df = pd.read_csv('d:\\\\telcon.csv',header=None)\n",
    "#df = df.iloc[:100]\n",
    "xy = df.values\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.02\n",
    "iterations = 1000\n",
    "\n",
    "print(\"seq_length:{}\\t data_dim:{}\\t hidden_dim:{}\\t learning_rate:{}\\t iterations:{}\"\\\n",
    "      .format(seq_length,data_dim,hidden_dim,learning_rate,iterations), file = f)\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "#xy = np.loadtxt('d:\\\\kkang.csv', delimiter=',')\n",
    "#xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "pre_train_set = xy[0:train_size]\n",
    "pre_test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainx, trainy = build_dataset(pre_train_set, seq_length)\n",
    "print(\"#\"*100)\n",
    "testx, testy = build_dataset(pre_test_set, seq_length)\n",
    "lastx = ([[[9.900e+03, 1.030e+04, 9.610e+03, 2.990e+06, 1.025e+04],\n",
    " [1.055e+04, 1.060e+04, 9.980e+03, 2.180e+06, 1.010e+04],\n",
    " [1.015e+04, 1.040e+04, 9.960e+03, 2.610e+06, 9.960e+03],\n",
    " [9.960e+03, 1.025e+04, 9.920e+03, 2.960e+06, 1.015e+04],\n",
    " [1.015e+04, 1.025e+04, 9.970e+03, 1.910e+06, 1.025e+04],\n",
    " [1.030e+04, 1.135e+04, 1.000e+04, 1.155e+07, 1.120e+04],\n",
    " [1.115e+04, 1.140e+04, 1.030e+04, 4.700e+06, 1.060e+04]]])\n",
    "\n",
    "# Scale each\n",
    "trainX = MinMaxScaler(trainx)\n",
    "trainY = MinMaxScaler(trainy)\n",
    "testX = MinMaxScaler(testx)\n",
    "lastX = MinMaxScaler(lastx)\n",
    "testY = MinMaxScaler(testy)\n",
    "\n",
    "# Scale each\n",
    "trainX = MinMaxScaler(trainx)\n",
    "trainY = MinMaxScaler(trainy)\n",
    "testX = MinMaxScaler(testx)\n",
    "testY = MinMaxScaler(testy)\n",
    "\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    \n",
    "    # Predictions test\n",
    "    prediction_last = sess.run(Y_pred, feed_dict={X: last_X.reshape(1, 7, 5)})\n",
    "    #prediction_last = sess.run(Y_pred, feed_dict={X: last_X})\n",
    "    print(\"prediction_last:{}\".format(last_predict))\n",
    "    \n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "print(prediction_last *test_denom[pred_label]+test_min[pred_label])    \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Keras_LSTM_Stock_Prediction#1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "df = pd.read_excel(\"d:\\\\hh.xlsx\")\n",
    "df=df[['Open','High','Close']]\n",
    "\n",
    "\n",
    "df['High'] = df['High'] / 100000\n",
    "df['Open'] = df['Open'] / 100000\n",
    "df['Close'] = df['Close'] / 100000\n",
    "df.head(5)\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.values #pd.DataFrame(stock)\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "        model.add(Dense(1,kernel_initializer=\"uniform\",activation='relu'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "window = 5\n",
    "X_train, y_train, X_test, y_test = load_data(df[:], window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "\n",
    "# model = build_model([3,lag,1])\n",
    "model = build_model2([3,window,1])\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=10,epochs=100,validation_split=0.1,verbose=0)\n",
    "\n",
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "\n",
    "# print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))\n",
    "\n",
    "plt.plot(p,color='red', label='prediction')\n",
    "plt.plot(y_test,color='blue', label='y_test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Keras_LSTM_Stock_Prediction#1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "df = pd.read_excel(\"d:\\\\hh.xlsx\")\n",
    "df=df[['Open','High','Close']]\n",
    "\n",
    "df['High'] = df['High'] / 100000\n",
    "df['Open'] = df['Open'] / 100000\n",
    "df['Close'] = df['Close'] / 100000\n",
    "df.head(5)\n",
    "\n",
    "window = 5\n",
    "amount_of_features = len(df.columns)\n",
    "data = df.values #pd.DataFrame(stock)\n",
    "sequence_length = window + 1\n",
    "result = []\n",
    "\n",
    "for index in range(len(data) - sequence_length):\n",
    "    result.append(data[index: index + sequence_length])\n",
    "\n",
    "result = np.array(result)\n",
    "row = round(0.9 * result.shape[0])\n",
    "train = result[:int(row), :]\n",
    "x_train = train[:, :-1]\n",
    "y_train = train[:, -1][:,-1]\n",
    "x_test = result[int(row):, :-1]\n",
    "y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "\n",
    "\n",
    "d = 0.2\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(window, 3), return_sequences=True))\n",
    "model.add(Dropout(d))\n",
    "model.add(LSTM(64, input_shape=(window, 3), return_sequences=False))\n",
    "model.add(Dropout(d))\n",
    "model.add(Dense(16,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "model.add(Dense(1,kernel_initializer=\"uniform\",activation='relu'))\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,batch_size=10,epochs=100,validation_split=0.1,verbose=0)\n",
    "\n",
    "trainScore = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "\n",
    "# print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(x_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))\n",
    "\n",
    "plt.plot(p,color='red', label='prediction')\n",
    "plt.plot(y_test,color='blue', label='y_test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stock_Prediction_with_LSTM(Open,High,Low,Volumw,Close)\n",
    "### period(One_year) seq_length = 7 optimizer=gradient\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "f = open('stock_prediction with LSTM_version-2.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_One_Year =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.002\n",
    "iterations = 10000\n",
    "LSTM_stack = 1\n",
    "gradient = \"tf.train.GradientDescentOptimizer\"\n",
    "adam = \"tf.train.AdamOptimizer\"\n",
    "\n",
    "# Choose stock\n",
    "stock = \"008770\"\n",
    "pred_label = 4 # 0 : Open, 1: High 2: Low, 3:Volume 4:Close\n",
    "\n",
    "# 날짜 지정\n",
    "now=datetime.now()\n",
    "year = now.year\n",
    "month = now.month\n",
    "day = now.day\n",
    "\n",
    "start = datetime(2018, 1, 2)\n",
    "end = datetime(year, month, day)\n",
    "df_stock = fdr.DataReader(stock, start, end)\n",
    "\n",
    "df = df_stock[df_stock.Volume > 0]\n",
    "df = df[['Open','High','Low','Volume','Close']]\n",
    "\n",
    "# Convert pandas dataframe to numpy array\n",
    "dataset_temp = df.values\n",
    "#dataset_temp = dataset_temp[:-1,:]\n",
    "\n",
    "print(\"seq_length:{}\\t data_dim:{}\\t hidden_dim:{}\\t learning_rate:{}\\t iterations:{}\\t len_data_set:{}\"\\\n",
    "      .format(seq_length,data_dim,hidden_dim,learning_rate,iterations,len(dataset_temp)), file = f)\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "test_min = np.min(dataset_temp, 0)\n",
    "test_max = np.max(dataset_temp, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(dataset_temp)\n",
    "\n",
    "#최종 7 rows 다음날 Price Predict 입력 data\n",
    "test_last_X = (dataset_temp[-seq_length:,:]-test_min)/(test_denom+1e-7);\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataY_temp = []\n",
    "for i in range(0, len(dataset) - seq_length):\n",
    "    _x = dataset[i:i + seq_length]\n",
    "    #_y = dataset_label[i + seq_length]  # Next close price\n",
    "    _y = dataset[i + seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "\n",
    "# train/test split 70 / 30\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='input_X')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='intput_Y')\n",
    "\n",
    "# build a LSTM network\n",
    "def lstm_cell():\n",
    "    cell_temp = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    return cell_temp\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(LSTM_stack)], state_is_tuple=True)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y), name='losses_sum')  # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, name='train')\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "predictions = tf.placeholder(tf.float32, [None, 1], name='predictions')\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)), name='rmse')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Tensorboard\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./tensorflowlog\", sess.graph)\n",
    "\n",
    "    losslist = [];\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY[:, [pred_label]]})\n",
    "        losslist = np.append(losslist, step_loss)\n",
    "        if i%1000 == 0:\n",
    "            print(\"step_loss : {}\".format(step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse = sess.run(rmse, feed_dict={\n",
    "        targets: testY[:, [pred_label]], predictions: test_predict})\n",
    "\n",
    "    # Predictions test\n",
    "    prediction_last = sess.run(Y_pred, feed_dict={X: test_last_X.reshape(1, 7, 5)})\n",
    "    \n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse))\n",
    "print(\"predict_price:{}\".format(prediction_last *test_denom[pred_label]+test_min[pred_label]))    \n",
    "\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stock_Prediction_with_LSTM(Open,High,Low,Volume,foreign,fund,Close)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "from datetime import datetime\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 5\n",
    "data_dim = 7\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learing_rate = 0.002\n",
    "iterations = 10000\n",
    "LSTM_stack = 1\n",
    "\n",
    "\n",
    "# Choose stock\n",
    "stock = \"008770\"\n",
    "pred_label = 6 # 0 : Open, 1: High 2: Low, 3:foreign, 4:fund, 5:Volume, 6:Close\n",
    "\n",
    "# 날짜 지정\n",
    "now=datetime.now()\n",
    "year = now.year\n",
    "month = now.month\n",
    "day = now.day\n",
    "\n",
    "start = datetime(2018, 1, 2)\n",
    "end = datetime(year, month, day)\n",
    "df_stock = fdr.DataReader(stock, start, end)\n",
    "\n",
    "df = df_stock[df_stock.Volume > 0]\n",
    "df = df[['Open','High','Low','Volume','Close']]\n",
    "df1 = pd.read_excel('d:\\\\hotel.xlsx')\n",
    "df = pd.merge(df,df1,on='Date')\n",
    "df = df[['Open','High','Low','foreign','fund','Volume','Close']]\n",
    "\n",
    "# Convert pandas dataframe to numpy array\n",
    "dataset_temp = df.values\n",
    "#dataset_temp = dataset_temp[:-1,:]\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "test_min = np.min(dataset_temp, 0)\n",
    "test_max = np.max(dataset_temp, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(dataset_temp)\n",
    "\n",
    "#최종 7 rows 다음날 Price Predict 입력 data\n",
    "test_last_X = (dataset_temp[-seq_length:,:]-test_min)/(test_denom+1e-7);\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataY_temp = []\n",
    "for i in range(0, len(dataset) - seq_length):\n",
    "    _x = dataset[i:i + seq_length]\n",
    "    #_y = dataset_label[i + seq_length]  # Next close price\n",
    "    _y = dataset[i + seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "\n",
    "# train/test split 70 / 30\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='input_X')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='intput_Y')\n",
    "\n",
    "# build a LSTM network\n",
    "def lstm_cell():\n",
    "    cell_temp = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    return cell_temp\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(LSTM_stack)], state_is_tuple=True)\n",
    "\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y), name='losses_sum')  # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learing_rate)\n",
    "train = optimizer.minimize(loss, name='train')\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "predictions = tf.placeholder(tf.float32, [None, 1], name='predictions')\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)), name='rmse')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Tensorboard\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./tensorflowlog\", sess.graph)\n",
    "\n",
    "    losslist = [];\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY[:, [pred_label]]})\n",
    "        losslist = np.append(losslist, step_loss)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"step_loss : {}\".format(step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse = sess.run(rmse, feed_dict={targets: testY[:, [pred_label]], predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "    # Predictions test\n",
    "    prediction_test = sess.run(Y_pred, feed_dict={X: test_last_X.reshape(1, 5, 7)})\n",
    "    \n",
    "\n",
    "print(\"predictions \", end='')\n",
    "print(prediction_test *test_denom[pred_label]+test_min[pred_label])  \n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(2)\n",
    "testY_plt=testY[:, [pred_label]]*test_denom[pred_label]+test_min[pred_label]\n",
    "test_predict_plt=test_predict*test_denom[pred_label]+test_min[pred_label]\n",
    "plt.plot(testY_plt, color=\"red\", label=\"Real\")\n",
    "plt.plot(test_predict_plt, color=\"blue\", label=\"Prediction\")\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\" 복조및 Close, Predict 비교\n",
    "Total_size=df.values.shape[0]\n",
    "print(Total_size)\n",
    "\n",
    "Predict_size=test_predict_plt.shape[0]\n",
    "print(Predict_size)\n",
    "\n",
    "Predict_close_concat=np.concatenate((np.zeros((Total_size-Predict_size,1)) ,test_predict_plt), axis=0)\n",
    "df[\"Predict Close\"]=Predict_close_concat.round()\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stock_Prediction_with_LSTM(Open,Volume,tense,buy,ant,foreign,fund,rent,gong,Close)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import FinanceDataReader as fdr\n",
    "from datetime import datetime\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 5\n",
    "data_dim = 10\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learing_rate = 0.002\n",
    "iterations = 10000\n",
    "LSTM_stack = 2\n",
    "\n",
    "\n",
    "# Choose stock\n",
    "stock = \"008770\"\n",
    "pred_label = 9 # 0 : Open, 1: Volume ,2: Tense, 3: buy, 4: ant, 5:foreign, 6:fund, 7: rent, 8: gong, 9:Close\n",
    "\n",
    "# 날짜 지정\n",
    "now=datetime.now()\n",
    "year = now.year\n",
    "month = now.month\n",
    "day = now.day\n",
    "\n",
    "#start = datetime(2018, 1, 2)\n",
    "#end = datetime(year, month, day)\n",
    "#df_stock = fdr.DataReader(stock, start, end)\n",
    "\n",
    "#df = df_stock[df_stock.Volume > 0]\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = pd.read_excel('d:\\\\hotel_a.xlsx')\n",
    "#df = pd.merge(df,df1,on='Date')\n",
    "#df = df[['Open','High','Low','foreign','fund','Volume','Close']]\n",
    "\n",
    "# Convert pandas dataframe to numpy array\n",
    "dataset_temp = df.values\n",
    "#dataset_temp = dataset_temp[:-1,:]\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "test_min = np.min(dataset_temp, 0)\n",
    "test_max = np.max(dataset_temp, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(dataset_temp)\n",
    "\n",
    "#최종 7 rows 다음날 Price Predict 입력 data\n",
    "test_last_X = (dataset_temp[-seq_length:,:]-test_min)/(test_denom+1e-7);\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataY_temp = []\n",
    "for i in range(0, len(dataset) - seq_length):\n",
    "    _x = dataset[i:i + seq_length]\n",
    "    #_y = dataset_label[i + seq_length]  # Next close price\n",
    "    _y = dataset[i + seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "\n",
    "# train/test split 70 / 30\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])\n",
    "\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='input_X')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='intput_Y')\n",
    "\n",
    "# build a LSTM network\n",
    "def lstm_cell():\n",
    "    cell_temp = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    return cell_temp\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(LSTM_stack)], state_is_tuple=True)\n",
    "\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y), name='losses_sum')  # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learing_rate)\n",
    "train = optimizer.minimize(loss, name='train')\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "predictions = tf.placeholder(tf.float32, [None, 1], name='predictions')\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)), name='rmse')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Tensorboard\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./tensorflowlog\", sess.graph)\n",
    "\n",
    "    losslist = [];\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY[:, [pred_label]]})\n",
    "        losslist = np.append(losslist, step_loss)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"step_loss : {}\".format(step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse = sess.run(rmse, feed_dict={targets: testY[:, [pred_label]], predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse))\n",
    "\n",
    "    # Predictions test\n",
    "    prediction_test = sess.run(Y_pred, feed_dict={X: test_last_X.reshape(1, 5, 10)})\n",
    "    \n",
    "\n",
    "print(\"predictions \", end='')\n",
    "print(prediction_test *test_denom[pred_label]+test_min[pred_label])   \n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(2)\n",
    "testY_plt=testY[:, [pred_label]]*test_denom[pred_label]+test_min[pred_label]\n",
    "test_predict_plt=test_predict*test_denom[pred_label]+test_min[pred_label]\n",
    "plt.plot(testY_plt, color=\"red\", label=\"Real\")\n",
    "plt.plot(test_predict_plt, color=\"blue\", label=\"Prediction\")\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Total_size=df.values.shape[0]\n",
    "Total_size\n",
    "\n",
    "Predict_size=test_predict_plt.shape[0]\n",
    "Predict_size\n",
    "\n",
    "Predict_close_concat=np.concatenate((np.zeros((Total_size-Predict_size,1)) ,test_predict_plt), axis=0)\n",
    "df[\"Predict Close\"]=Predict_close_concat.round()\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Save - MinMaxScaler.fit \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 9000\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='first_input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"first_output\")\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "Y_pred = tf.identity(Y_pred, \"model\")\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    saver.save(sess, './first_model/first', global_step=1000)\n",
    "    \n",
    "    # Predictions test\n",
    "    #prediction_last = sess.run(Y_pred, feed_dict={X: lastX.reshape(1,seq_length,data_dim)})\n",
    "\n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "#print(\"prediction_last :{}\". format(prediction_last) )\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Model Restore - MinMaxScaler.fit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# input place holders\n",
    "#X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='first_input')  *** 중요 !!! restore에 name을 주면 안됨\n",
    "#Y = tf.placeholder(tf.float32, [None, 1], name=\"first_output\")\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('first_model/first-1000.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./first_model'))\n",
    "\n",
    "X = sess.graph.get_tensor_by_name(\"first_input:0\")\n",
    "Y = sess.graph.get_tensor_by_name(\"first_output:0\")\n",
    "Y_pred = sess.graph.get_tensor_by_name(\"model:0\")\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "print(\"test_predict:{}\".format(test_predict))\n",
    "\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
