{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-87bcdb1de5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mtotalPage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'totalPages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotalPage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 582\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kkang\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# 2019-01-28 수정\n",
    "# 기존 daum 주식 사이트 : ajax 방식으로 변경으로 인해 이를 반영한 코드를 수정.\n",
    "# pip install fake-useragent 설치 후 실행 가능\n",
    "\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import urllib.request as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "#sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=1\"\n",
    "html = requests.get(url, headers = headers).text\n",
    "\n",
    "# 헤더 선언\n",
    "#headers = {\n",
    "#    'User-Agent': ua.ie,\n",
    "#    'referer': 'http://finance.daum.net/domestic/futures'\n",
    "#}\n",
    "\n",
    "\n",
    "url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=1\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "df1 = pd.DataFrame()\n",
    "for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=\"+str(i)\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accTradeVolume', 'change', 'changePrice', 'changeRate', 'date',\n",
       "       'foreignSettlement', 'institutionSettlement', 'privateSettlement',\n",
       "       'tradePrice', 'unsettledVolume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-01-28 수정\n",
    "# 기존 daum 주식 사이트 : ajax 방식으로 변경으로 인해 이를 반영한 코드를 수정.\n",
    "# pip install fake-useragent 설치 후 실행 가능\n",
    "\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "import urllib.request as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "#sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "# 헤더 선언\n",
    "headers = {\n",
    "    'User-Agent': ua.ie,\n",
    "    'referer': 'http://finance.daum.net/domestic/futures'\n",
    "}\n",
    "\n",
    "\n",
    "url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=1\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "df1 = pd.DataFrame()\n",
    "for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=\"+str(i)\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "\n",
    "df2 = df1[['date','tradePrice','change', 'changePrice','changeRate','unsettledVolume','foreignSettlement', 'institutionSettlement', 'privateSettlement']]\n",
    "df2.columns=('Date','Close','change','가격변동','등락률','미결제약정','외국인','기관','개인')\n",
    "df2['Date'] = pd.to_datetime(df2['Date']).dt.date\n",
    "#df2['Date'] = pd.to_datetime(df2['Date']).apply(lambda x: x.date())\n",
    "#df2['Date'] = pd.to_datetime(df2['Date'], format = '%Y-%m-%d') # yyyy-mm-dd hh:mm:ss -> yyyy-mm-dd (속성은그대로 보여주는 형식만 변경)\n",
    "df2 =df2[['Date','Close','미결제약정','외국인','기관','개인']]\n",
    "df2.to_sql(name='future', con=engine, if_exists='append', index = False)\n",
    "df2 = df2.set_index('Date')\n",
    "df2.to_excel('d:\\\\future.xlsx',encoding='utf-8')\n",
    "df2\n",
    "            \n",
    "                    market_df = pd.read_sql(\"select Date from market order by Date desc limit 1\", engine)\n",
    "                    market_df = str(market_df['Date'])\n",
    "                    print(market_df)\n",
    "                    start_date =  market_df[5:15]\n",
    "                    year = start_date.split('-')[0]\n",
    "                    mm = start_date.split('-')[1]\n",
    "                    dd = start_date.split('-')[2]\n",
    "                    dd = int(dd)+1\n",
    "                    dd = str(dd)\n",
    "                    \n",
    "                    #year=year[2:]\n",
    "                    start_date = year+'-'+mm+'-'+dd           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "import urllib.request as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "# 헤더 선언\n",
    "headers = {\n",
    "    'User-Agent': ua.ie,\n",
    "    'referer': 'http://finance.daum.net/domestic/futures'\n",
    "}\n",
    "\n",
    "url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=1\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "def future_with_date(self,until_date=real_yesterday,type=1):\n",
    "    \n",
    "    for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=\"+str(i)\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\future.xlsx'\n",
    "        \n",
    "        if type == 1:\n",
    "            until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        else:\n",
    "            kpi200_df = pd.read_sql(\"select Date from kpi200 order by Date desc limit 1\", engine)\n",
    "            kpi200_df = str(kpi200_df['Date'])\n",
    "            until_date = kpi200_df[5:15]\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['개인','외국인','기관']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('d:\\\\future.xlsx')\n",
    "df=df[['Date','Close']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "from datetime import datetime,timedelta\n",
    "from urllib.request import urlopen\n",
    "import sqlalchemy \n",
    "import pymysql\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "today = datetime.now()\n",
    "real_yesterday = (today-timedelta(1)).strftime('%Y-%m-%d')\n",
    "real_today = today.strftime('%Y-%m-%d')\n",
    "\n",
    "now = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "\n",
    "\n",
    "class to_excel:\n",
    "    \n",
    "    def get_investor_trend(self):\n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['개인']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['개인'].pop(-1)\n",
    "                    dictionary['외국인'].pop(-1)\n",
    "                    dictionary['기관'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['개인']))\n",
    "        #print(len(dictionary['외국인']))\n",
    "        #print(len(dictionary['기관']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df = df[['개인','외국인','기관']]\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "        \n",
    "    def get_investor_trend_date(self,until_date=real_yesterday):\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "\n",
    "        until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "    \n",
    "        year = until_date.split('-')[0]\n",
    "        mm = until_date.split('-')[1]\n",
    "        dd = until_date.split('-')[2]\n",
    "        year=year[2:]\n",
    "        until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['개인','외국인','기관']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1\n",
    "                    \n",
    "class to_sql:\n",
    "    \n",
    "    def excel_to_sql(self, type = 1):\n",
    "        excel_name_list=['kpi200.xlsx', 'investortrend.xlsx','programtrend.xlsx','moneytrend.xlsx']\n",
    "        sql_table_name_list=['kpi200','investortrend','programtrend','moneytrend']\n",
    "\n",
    "        if type == 1:\n",
    "        \n",
    "            file_name = input('파일이름을 입력하세요:')\n",
    "\n",
    "            df=pd.read_excel('d:\\\\'+ file_name)\n",
    "            if file_name=='kpi200.xlsx':\n",
    "                table_name = 'kpi200'\n",
    "                df.columns=['Date','kpi200','거래량']\n",
    "\n",
    "            elif file_name=='investortrend.xlsx':\n",
    "                table_name = 'investortrend'\n",
    "                df.columns=['Date', '개인', '외국인','기관']\n",
    "\n",
    "            elif file_name=='moneytrend.xlsx':\n",
    "                table_name = 'moneytrend'\n",
    "                df.columns=['Date', '고객예탁금', '신용잔고','주식형펀드','혼합형펀드','채권형펀드']\n",
    "\n",
    "            elif file_name=='programtrend.xlsx':\n",
    "                table_name = 'programtrend'\n",
    "                df.columns=['Date', '차익', '비차익','전체']\n",
    "\n",
    "            elif file_name=='market.xlsx':\n",
    "                data = pd.read_excel('d:\\\\market.xlsx')\n",
    "                start_date = input(\"시작날자를 입려하세요 : sample: '2015-01-01'\")\n",
    "\n",
    "                code_list = data['종목코드'].tolist()\n",
    "                code_list = [str(item).zfill(6) for item in code_list]\n",
    "                name_list = data['종목명'].tolist()\n",
    "\n",
    "                # 코스피 상장종목 전체\n",
    "                stock_dic = dict(list(zip(code_list,name_list)))\n",
    "\n",
    "                for code in sorted(stock_dic.keys()):\n",
    "                    df  = fdr.DataReader(code,start_date)\n",
    "                    print(code,stock_dic[code])\n",
    "                    df['Code'],df['Name'] = code,stock_dic[code]\n",
    "                    df = df[['Code','Name','Open','High','Low','Volume','Close']]\n",
    "                    df.to_sql(name='market', con=engine, if_exists='append')\n",
    "                return \n",
    "\n",
    "            else:\n",
    "                print('\\n file_name error\\n')\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "            print(df)\n",
    "            \n",
    "        else :\n",
    "            a = 0\n",
    "            for i in excel_name_list:\n",
    "\n",
    "                table_name = sql_table_name_list[a]\n",
    "                df=pd.read_excel('d:\\\\'+ i)\n",
    "                print(table_name)\n",
    "                #print(df.columns)\n",
    "                #print(df['Unnamed: 0'])\n",
    "                df = df.rename(columns = {'Unnamed: 0': 'Date'})\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "                print(df)\n",
    "                a += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_investor_trend_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.until_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_yesterday = '2019-08-19'\n",
    "\n",
    "def get_kpi200_date(self,until_date=real_yesterday):\n",
    "    \n",
    "    url = 'https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page='\n",
    "\n",
    "    source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "    source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "    last = source.find('td',class_='pgRR').find('a')['href']\n",
    "    last = last.split('page')[1]\n",
    "    last = last.split('=')[1]\n",
    "    last = int(last)\n",
    "    print(last)\n",
    "\n",
    "    # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "    path = 'd:\\\\kpi200.xlsx'\n",
    "\n",
    "    until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    " \n",
    "\n",
    "connect = create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "df = pd.read_sql(\"select Date from kpi200 order by Date desc limit 1\", connect)\n",
    "\n",
    "df = str(df['Date'])\n",
    "\n",
    "def get_investor_trend_date():\n",
    "\n",
    "    until_date = df[5:15]\n",
    "\n",
    "    year = until_date.split('-')[0]\n",
    "\n",
    "    mm = until_date.split('-')[1]\n",
    "\n",
    "    dd = until_date.split('-')[2]\n",
    "\n",
    "    year=year[2:]\n",
    "\n",
    "    until_date = year+'-'+mm+'-'+dd\n",
    "\n",
    "    print('year:{}'.format(year))\n",
    "\n",
    "    print('until_date:{}'.format(until_date))\n",
    "    \n",
    "    print('len_year.{}'.format(len(year)))\n",
    "    print('len_until.{}'.format(len(until_date)))\n",
    "    print('until_date.{}'.format)\n",
    "\n",
    "    \n",
    "\n",
    "get_investor_trend_date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from int_date import get_date_from_diff\n",
    "\n",
    "__author__ = 'Cedric Zhuang'\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class StockDataFrame(pd.DataFrame):\n",
    "    OPERATORS = ['le', 'ge', 'lt', 'gt', 'eq', 'ne']\n",
    "\n",
    "    # Start of options.\n",
    "    KDJ_PARAM = (2.0 / 3.0, 1.0 / 3.0)\n",
    "    KDJ_WINDOW = 9\n",
    "\n",
    "    BOLL_PERIOD = 20\n",
    "    BOLL_STD_TIMES = 2\n",
    "\n",
    "    MACD_EMA_SHORT = 12\n",
    "    MACD_EMA_LONG = 26\n",
    "    MACD_EMA_SIGNAL = 9\n",
    "\n",
    "    PDI_SMMA = 14\n",
    "    MDI_SMMA = 14\n",
    "    DX_SMMA = 14\n",
    "    ADX_EMA = 6\n",
    "    ADXR_EMA = 6\n",
    "\n",
    "    CR_MA1 = 5\n",
    "    CR_MA2 = 10\n",
    "    CR_MA3 = 20\n",
    "\n",
    "    TRIX_EMA_WINDOW = 12\n",
    "\n",
    "    TEMA_EMA_WINDOW = 5\n",
    "\n",
    "    ATR_SMMA = 14\n",
    "\n",
    "    # End of options\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_change(df):\n",
    "        df['change'] = df['Close'].pct_change() * 100\n",
    "        return df['change']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    " \n",
    "\n",
    "connect = create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "df = pd.read_sql(\"select Close from market where Name='hrs' and Date > '2019-08-01'\", connect)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = StockDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a._get_change(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-01-28 수정\n",
    "# 기존 daum 주식 사이트 : ajax 방식으로 변경으로 인해 이를 반영한 코드를 수정.\n",
    "# pip install fake-useragent 설치 후 실행 가능\n",
    "\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "#sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "# 헤더 선언\n",
    "headers = {\n",
    "    'User-Agent': ua.ie,\n",
    "    'referer': 'http://finance.daum.net/domestic/futures'\n",
    "}\n",
    "\n",
    "\n",
    "url = \"http://finance.daum.net/api/future/KR4101P60004/days?pagination=true&page=1\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "df1 = pd.DataFrame()\n",
    "for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=\"+str(i)\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "\n",
    "df2 = df1[['date','tradePrice','change', 'changePrice','changeRate','unsettledVolume','foreignSettlement', 'institutionSettlement', 'privateSettlement']]\n",
    "df2.columns=('Date','종가','change','가격변동','등락률','미결제수량','외국인','기관','개인')\n",
    "#df2 = df2.set_index('date')\n",
    "df2\n",
    "df2.to_excel('d:\\\\future.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = pd.to_datetime(df['col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('d:\\\\future.xlsx', parse_dates=['Date'], dayfirst=True)\n",
    "#df = pd.read_excel('d:\\\\future.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_sql(name='future', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-01-28 수정\n",
    "# 기존 daum 주식 사이트 : ajax 방식으로 변경으로 인해 이를 반영한 코드를 수정.\n",
    "# pip install fake-useragent 설치 후 실행 가능\n",
    "\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "#sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "# 헤더 선언\n",
    "headers = {\n",
    "    'User-Agent': ua.ie,\n",
    "    'referer': 'http://finance.daum.net/domestic/futures'\n",
    "}\n",
    "\n",
    "\n",
    "url = \"http://finance.daum.net/api/future/KR4101P60004/days?pagination=true&page=1\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "df1 = pd.DataFrame()\n",
    "for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/future/KR4101P90001/days?pagination=true&page=\"+str(i)\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-01-28 수정\n",
    "# 기존 daum 주식 사이트 : ajax 방식으로 변경으로 인해 이를 반영한 코드를 수정.\n",
    "# pip install fake-useragent 설치 후 실행 가능\n",
    "\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import urllib.request as req\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')\n",
    "#sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')\n",
    "\n",
    "# Fake Header 정보\n",
    "ua = UserAgent()\n",
    "\n",
    "# 헤더 선언\n",
    "headers = {\n",
    "    'User-Agent': ua.ie,\n",
    "    'referer': 'http://finance.daum.net/domestic/themes'\n",
    "}\n",
    "\n",
    "url = \"http://finance.daum.net/api/themes/leading_stocks?page=1&perPage=30&fieldName=changeRate&order=desc&pagination=true\"\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "totalPage = json.loads(res)['totalPages']\n",
    "num = totalPage+1\n",
    "\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "df1 = pd.DataFrame()\n",
    "for i in range(1,num):\n",
    "    # 다음 주식 요청 URL\n",
    "    url = \"http://finance.daum.net/api/themes/leading_stocks?page=\"+str(i)+\"&perPage=30&fieldName=changeRate&order=desc&pagination=true\"\n",
    "\n",
    "    res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "    \n",
    "    rank_json = json.loads(res)['data']\n",
    "    \n",
    "    df = pd.DataFrame(rank_json)\n",
    "    df1 = df1.append(df,ignore_index=True)\n",
    "\n",
    "thema_name=[]\n",
    "for i in range(len(df1.index)):\n",
    "    stock_name = [df1['leadingStocks'][i][0]['name'],df1['leadingStocks'][i][1]['name']]\n",
    "    #print(stock_name)\n",
    "    thema_name.append(stock_name)\n",
    "thema_name_df=pd.DataFrame(thema_name)\n",
    "\n",
    "df1 = df1[['name','changeRate','metadata']]\n",
    "df1['changeRate'] = df1['changeRate']*100\n",
    "\n",
    "thema_df = df1.join(thema_name_df)\n",
    "\n",
    "thema_df = thema_df.sort_values(['changeRate'], ascending=[False])\n",
    "thema_df.columns=('area','change','spread','fist','two')\n",
    "\n",
    "thema_df.to_excel('d:\\\\future.xlsx',encoding='utf-8')\n",
    "thema_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thema_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "\n",
    "class to_sql:\n",
    "    \n",
    "    def excel_to_sql(self, type = 1):\n",
    "        excel_name_list=['kpi200.xlsx', 'investortrend.xlsx','programtrend.xlsx','moneytrend.xlsx','market.xlsx']\n",
    "        sql_table_name_list=['kpi200','investortrend','programtrend','moneytrend','market']\n",
    "\n",
    "        if type == 1:\n",
    "        \n",
    "            file_name = input('파일이름을 입력하세요:')\n",
    "\n",
    "            df=pd.read_excel('d:\\\\'+ file_name)\n",
    "            if file_name=='kpi200.xlsx':\n",
    "                table_name = 'kpi200'\n",
    "                df.columns=['Date','kpi200','거래량']\n",
    "\n",
    "            elif file_name=='investortrend.xlsx':\n",
    "                table_name = 'investortrend'\n",
    "                df.columns=['Date', '개인', '외국인','기관']\n",
    "\n",
    "            elif file_name=='moneytrend.xlsx':\n",
    "                table_name = 'moneytrend'\n",
    "                df.columns=['Date', '고객예탁금', '신용잔고','주식형펀드','혼합형펀드','채권형펀드']\n",
    "\n",
    "            elif file_name=='programtrend.xlsx':\n",
    "                table_name = 'programtrend'\n",
    "                df.columns=['Date', '차익', '비차익','전체']\n",
    "\n",
    "            elif file_name=='market.xlsx':\n",
    "                data = pd.read_excel('d:\\\\market.xlsx')\n",
    "                start_date = input(\"시작날자를 입려하세요 : sample: '2015-01-01'\")\n",
    "\n",
    "                code_list = data['종목코드'].tolist()\n",
    "                code_list = [str(item).zfill(6) for item in code_list]\n",
    "                name_list = data['종목명'].tolist()\n",
    "\n",
    "                # 코스피 상장종목 전체\n",
    "                stock_dic = dict(list(zip(code_list,name_list)))\n",
    "\n",
    "                for code in sorted(stock_dic.keys()):\n",
    "                    df  = fdr.DataReader(code,start_date)\n",
    "                    print(code,stock_dic[code])\n",
    "                    df['Code'],df['Name'] = code,stock_dic[code]\n",
    "                    df = df[['Code','Name','Open','High','Low','Volume','Close']]\n",
    "                    df.to_sql(name='market', con=engine, if_exists='append')\n",
    "                return \n",
    "\n",
    "            else:\n",
    "                print('\\n file_name error\\n')\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "            print(df)\n",
    "            \n",
    "        else :\n",
    "            a = 0\n",
    "            for i in excel_name_list:\n",
    "                if i == 'market.xlsx':\n",
    "                    data = pd.read_excel('d:\\\\market.xlsx')\n",
    "                    market_df = pd.read_sql(\"select Date from market order by Date desc limit 1\", engine)\n",
    "                    market_df = str(market_df['Date'])\n",
    "                    print(market_df)\n",
    "                    start_date =  market_df[5:15]\n",
    "                    year = start_date.split('-')[0]\n",
    "                    mm = start_date.split('-')[1]\n",
    "                    dd = start_date.split('-')[2]\n",
    "                    dd = int(dd)+1\n",
    "                    dd = str(dd)\n",
    "                    \n",
    "                    #year=year[2:]\n",
    "                    start_date = year+'-'+mm+'-'+dd\n",
    "                    print('start_date:{}'.format(start_date))\n",
    "\n",
    "                    code_list = data['종목코드'].tolist()\n",
    "                    code_list = [str(item).zfill(6) for item in code_list]\n",
    "                    name_list = data['종목명'].tolist()\n",
    "\n",
    "                    # 코스피 상장종목 전체\n",
    "                    stock_dic = dict(list(zip(code_list,name_list)))\n",
    "\n",
    "                    for code in sorted(stock_dic.keys()):\n",
    "                        df  = fdr.DataReader(code,start_date)\n",
    "                        print(code,stock_dic[code])\n",
    "                        df['Code'],df['Name'] = code,stock_dic[code]\n",
    "                        df = df[['Code','Name','Open','High','Low','Volume','Close']]\n",
    "                        #df\n",
    "                        df.to_sql(name='market', con=engine, if_exists='append')\n",
    "                    return \n",
    "                else :\n",
    "                    table_name = sql_table_name_list[a]\n",
    "                    df=pd.read_excel('d:\\\\'+ i)\n",
    "                    print(table_name)\n",
    "                    df = df.rename(columns = {'Unnamed: 0': 'Date'})\n",
    "                    #df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "                    print(df)\n",
    "                a += 1\n",
    "                print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = to_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.excel_to_sql(type=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
