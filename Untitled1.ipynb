{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-cbcd881cbb93>, line 226)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-cbcd881cbb93>\"\u001b[1;36m, line \u001b[1;32m226\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "from datetime import datetime,timedelta\n",
    "from urllib.request import urlopen\n",
    "import sqlalchemy \n",
    "import pymysql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "today = datetime.now()\n",
    "real_yesterday = (today-timedelta(1)).strftime('%Y-%m-%d')\n",
    "real_today = today.strftime('%Y-%m-%d')\n",
    "\n",
    "now = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "\n",
    "\n",
    "class to_excel:\n",
    "    \n",
    "    def get_investor_trend(self):\n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['개인']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['개인'].pop(-1)\n",
    "                    dictionary['외국인'].pop(-1)\n",
    "                    dictionary['기관'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['개인']))\n",
    "        #print(len(dictionary['외국인']))\n",
    "        #print(len(dictionary['기관']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df = df[['개인','외국인','기관']]\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "        \n",
    "    def get_investor_trend_date(self,until_date=real_yesterday):\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "\n",
    "        until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "    \n",
    "        year = until_date.split('-')[0]\n",
    "        mm = until_date.split('-')[1]\n",
    "        dd = until_date.split('-')[2]\n",
    "        year=year[2:]\n",
    "        until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['개인','외국인','기관']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1\n",
    "                    \n",
    "class to_sql:\n",
    "    \n",
    "    def excel_to_sql(self, type = 1):\n",
    "        \n",
    "        if type == 1:\n",
    "        \n",
    "            file_name = input('파일이름을 입력하세요:')\n",
    "\n",
    "            df=pd.read_excel('d:\\\\'+ file_name)\n",
    "            if file_name=='kpi200.xlsx':\n",
    "                df.columns=['Date','kpi200','거래량']\n",
    "                table_name = 'kpi200'\n",
    "\n",
    "            elif file_name=='investortrend.xlsx':\n",
    "                table_name = 'investortrend'\n",
    "                df.columns=['Date', '개인', '외국인','기관']\n",
    "\n",
    "            elif file_name=='moneytrend.xlsx':\n",
    "                table_name = 'moneytrend'\n",
    "                df.columns=['Date', '고객예탁금', '신용잔고','주식형펀드','혼합형펀드','채권형펀드']\n",
    "\n",
    "            elif file_name=='programtrend.xlsx':\n",
    "                table_name = 'programtrend'\n",
    "                df.columns=['Date', '차익', '비차익','전체']\n",
    "\n",
    "            elif file_name=='market.xlsx':\n",
    "                data = pd.read_excel('d:\\\\market.xlsx')\n",
    "                start_date = input(\"시작날자를 입려하세요 : sample: '2015-01-01'\")\n",
    "\n",
    "                code_list = data['종목코드'].tolist()\n",
    "                code_list = [str(item).zfill(6) for item in code_list]\n",
    "                name_list = data['종목명'].tolist()\n",
    "\n",
    "                # 코스피 상장종목 전체\n",
    "                stock_dic = dict(list(zip(code_list,name_list)))\n",
    "\n",
    "                for code in sorted(stock_dic.keys()):\n",
    "                    df  = fdr.DataReader(code,start_date)\n",
    "                    print(code,stock_dic[code])\n",
    "                    df['Code'],df['Name'] = code,stock_dic[code]\n",
    "                    df = df[['Code','Name','Open','High','Low','Volume','Close']]\n",
    "                    df.to_sql(name='market', con=engine, if_exists='append')\n",
    "                return \n",
    "\n",
    "            else:\n",
    "                print('\\n file_name error\\n')\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "            print(df)\n",
    "            \n",
    "        else :\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "file = 'd:\\\\hrs.xlsx'\n",
    "\n",
    "connect = create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "df = pd.read_sql(\"SELECT * from market where Name = 'hrs' && Date > '2019-01-05'\", connect)\n",
    "\n",
    "volume_average_5 = df['Volume'].rolling(window=5).mean()\n",
    "volume_average_10 = df['Volume'].rolling(window=10).mean()\n",
    "volume_average_20 = df['Volume'].rolling(window=20).mean()\n",
    "volume_average_60 = df['Volume'].rolling(window=60).mean()\n",
    "volume_average_120 = df['Volume'].rolling(window=120).mean()\n",
    "\n",
    "close_average_5 = df['Close'].rolling(window=5).mean()\n",
    "close_average_10 = df['Close'].rolling(window=10).mean()\n",
    "close_average_20 = df['Close'].rolling(window=20).mean()\n",
    "close_average_60 = df['Close'].rolling(window=60).mean()\n",
    "close_average_120 = df['Close'].rolling(window=120).mean()\n",
    "\n",
    "df.insert(len(df.columns), \"Vol_MA5\", volume_average_5)\n",
    "df.insert(len(df.columns), \"Vol_MA10\", volume_average_10)\n",
    "df.insert(len(df.columns), \"Vol_MA20\", volume_average_20)\n",
    "df.insert(len(df.columns), \"Vol_MA60\", volume_average_60)\n",
    "df.insert(len(df.columns), \"Vol_MA120\", volume_average_120)\n",
    "\n",
    "df.insert(len(df.columns), \"Close_MA5\", close_average_5)\n",
    "df.insert(len(df.columns), \"Close_MA10\", close_average_10)\n",
    "df.insert(len(df.columns), \"Close_MA20\", close_average_20)\n",
    "df.insert(len(df.columns), \"Close_MA60\", close_average_60)\n",
    "df.insert(len(df.columns), \"Close_MA120\", close_average_120)\n",
    "\n",
    "df1 = df[['Date','Name','Close','Volume','Vol_MA5','Vol_MA10','Vol_MA20','Vol_MA60','Vol_MA120']]\n",
    "df1.to_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(df1['Volume'],df1['Close'])\n",
    "plt.show()\n",
    "\n",
    "#ax = df1[['Close','Vol_MA120']].plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "from datetime import datetime,timedelta\n",
    "from urllib.request import urlopen\n",
    "import sqlalchemy \n",
    "import pymysql\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "today = datetime.now()\n",
    "real_yesterday = (today-timedelta(1)).strftime('%Y-%m-%d')\n",
    "real_today = today.strftime('%Y-%m-%d')\n",
    "\n",
    "now = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "\n",
    "\n",
    "class to_excel:\n",
    "    \n",
    "    def get_investor_trend(self):\n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['개인']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['개인'].pop(-1)\n",
    "                    dictionary['외국인'].pop(-1)\n",
    "                    dictionary['기관'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['개인']))\n",
    "        #print(len(dictionary['외국인']))\n",
    "        #print(len(dictionary['기관']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df = df[['개인','외국인','기관']]\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "        \n",
    "    def get_investor_trend_date(self,until_date=real_yesterday):\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "\n",
    "        until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "    \n",
    "        year = until_date.split('-')[0]\n",
    "        mm = until_date.split('-')[1]\n",
    "        dd = until_date.split('-')[2]\n",
    "        year=year[2:]\n",
    "        until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['개인','외국인','기관']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1\n",
    "                    \n",
    "class to_sql:\n",
    "    \n",
    "    def excel_to_sql(self, type = 1):\n",
    "        excel_name_list=['kpi200.xlsx', 'investortrend.xlsx','programtrend.xlsx','moneytrend.xlsx']\n",
    "        sql_table_name_list=['kpi200','investortrend','programtrend','moneytrend']\n",
    "\n",
    "        if type == 1:\n",
    "        \n",
    "            file_name = input('파일이름을 입력하세요:')\n",
    "\n",
    "            df=pd.read_excel('d:\\\\'+ file_name)\n",
    "            if file_name=='kpi200.xlsx':\n",
    "                table_name = 'kpi200'\n",
    "                df.columns=['Date','kpi200','거래량']\n",
    "\n",
    "            elif file_name=='investortrend.xlsx':\n",
    "                table_name = 'investortrend'\n",
    "                df.columns=['Date', '개인', '외국인','기관']\n",
    "\n",
    "            elif file_name=='moneytrend.xlsx':\n",
    "                table_name = 'moneytrend'\n",
    "                df.columns=['Date', '고객예탁금', '신용잔고','주식형펀드','혼합형펀드','채권형펀드']\n",
    "\n",
    "            elif file_name=='programtrend.xlsx':\n",
    "                table_name = 'programtrend'\n",
    "                df.columns=['Date', '차익', '비차익','전체']\n",
    "\n",
    "            elif file_name=='market.xlsx':\n",
    "                data = pd.read_excel('d:\\\\market.xlsx')\n",
    "                start_date = input(\"시작날자를 입려하세요 : sample: '2015-01-01'\")\n",
    "\n",
    "                code_list = data['종목코드'].tolist()\n",
    "                code_list = [str(item).zfill(6) for item in code_list]\n",
    "                name_list = data['종목명'].tolist()\n",
    "\n",
    "                # 코스피 상장종목 전체\n",
    "                stock_dic = dict(list(zip(code_list,name_list)))\n",
    "\n",
    "                for code in sorted(stock_dic.keys()):\n",
    "                    df  = fdr.DataReader(code,start_date)\n",
    "                    print(code,stock_dic[code])\n",
    "                    df['Code'],df['Name'] = code,stock_dic[code]\n",
    "                    df = df[['Code','Name','Open','High','Low','Volume','Close']]\n",
    "                    df.to_sql(name='market', con=engine, if_exists='append')\n",
    "                return \n",
    "\n",
    "            else:\n",
    "                print('\\n file_name error\\n')\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "            print(df)\n",
    "            \n",
    "        else :\n",
    "            a = 0\n",
    "            for i in excel_name_list:\n",
    "\n",
    "                table_name = sql_table_name_list[a]\n",
    "                df=pd.read_excel('d:\\\\'+ i)\n",
    "                print(table_name)\n",
    "                #print(df.columns)\n",
    "                #print(df['Unnamed: 0'])\n",
    "                df = df.rename(columns = {'Unnamed: 0': 'Date'})\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='append', index = False)\n",
    "\n",
    "                print(df)\n",
    "                a += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class date_insert:\n",
    "\n",
    "\n",
    "    def excel_to_df(self, type = 1):\n",
    "        name_list=['kpi200.xlsx', 'moneytrend.xlsx']\n",
    "        name=['kpi200','moneytrend']\n",
    "        a = 0\n",
    "        for i in name_list:\n",
    "            \n",
    "            table_name = name[a]\n",
    "            df=pd.read_excel('d:\\\\'+ i)\n",
    "            print(table_name)\n",
    "            #print(df.columns)\n",
    "            #print(df['Unnamed: 0'])\n",
    "            df = df.rename(columns = {'Unnamed: 0': 'Date'})\n",
    "            print(df)\n",
    "            a += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = date_insert()\n",
    "a.excel_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.until_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_yesterday = '2019-08-19'\n",
    "\n",
    "def get_kpi200_date(self,until_date=real_yesterday):\n",
    "    \n",
    "    url = 'https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page='\n",
    "\n",
    "    source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "    source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "    last = source.find('td',class_='pgRR').find('a')['href']\n",
    "    last = last.split('page')[1]\n",
    "    last = last.split('=')[1]\n",
    "    last = int(last)\n",
    "    print(last)\n",
    "\n",
    "    # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "    path = 'd:\\\\kpi200.xlsx'\n",
    "\n",
    "    until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year:19\n",
      "until_date:19-08-22\n",
      "len_year.2\n",
      "len_until.8\n",
      "<built-in method format of str object at 0x0000000009AF1370>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    " \n",
    "\n",
    "connect = create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "df = pd.read_sql(\"select Date from kpi200 order by Date desc limit 1\", connect)\n",
    "\n",
    "df = str(df['Date'])\n",
    "\n",
    "def get_investor_trend_date():\n",
    "\n",
    "    until_date = df[5:15]\n",
    "\n",
    "    year = until_date.split('-')[0]\n",
    "\n",
    "    mm = until_date.split('-')[1]\n",
    "\n",
    "    dd = until_date.split('-')[2]\n",
    "\n",
    "    year=year[2:]\n",
    "\n",
    "    until_date = year+'-'+mm+'-'+dd\n",
    "\n",
    "    print('year:{}'.format(year))\n",
    "\n",
    "    print('until_date:{}'.format(until_date))\n",
    "    \n",
    "    print('len_year.{}'.format(len(year)))\n",
    "    print('len_until.{}'.format(len(until_date)))\n",
    "    print('until_date.{}'.format)\n",
    "\n",
    "    \n",
    "\n",
    "get_investor_trend_date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "\n",
    "import FinanceDataReader as fdr\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "from datetime import datetime,timedelta\n",
    "from urllib.request import urlopen\n",
    "import sqlalchemy \n",
    "import pymysql\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "today = datetime.now()\n",
    "real_yesterday = (today-timedelta(1)).strftime('%Y-%m-%d')\n",
    "real_today = today.strftime('%Y-%m-%d')\n",
    "\n",
    "now = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "\n",
    "class to_excel:\n",
    "    \n",
    "    def get_investortrend(self):\n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['개인']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['개인'].pop(-1)\n",
    "                    dictionary['외국인'].pop(-1)\n",
    "                    dictionary['기관'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['개인']))\n",
    "        #print(len(dictionary['외국인']))\n",
    "        #print(len(dictionary['기관']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df = df[['개인','외국인','기관']]\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "\n",
    "    def get_investor_trend_date(self,until_date=real_yesterday,type=1):\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/investorDealTrendDay.nhn?bizdate=2020601&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\investortrend.xlsx'\n",
    "        \n",
    "        if type == 1:\n",
    "            until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        else:\n",
    "            kpi200_df = pd.read_sql(\"select Date from kpi200 order by Date desc limit 1\", engine)\n",
    "            kpi200_df = str(kpi200_df['Date'])\n",
    "            until_date = kpi200_df[5:15]\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'개인': [],'외국인': [],'기관': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['개인','외국인','기관']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,2,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date2','rate_down3','rate_up3']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['개인','외국인','기관']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1\n",
    "    \n",
    "    def get_money_trend(self):\n",
    "    \n",
    "        url = 'http://finance.naver.com/sise/sise_deposit.nhn?&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('&')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\moneytrend.xlsx'   \n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'고객예탁금': [],'신용잔고': [],'주식형 펀드': [],'혼합형 펀드': [],'채권형 펀드': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['고객예탁금','신용잔고','주식형 펀드','혼합형 펀드','채권형 펀드']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,3,5,7,9]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','rate_down','rate_up']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int((count-1)/2)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(dictionary['고객예탁금']) != len(dictionary['채권형 펀드']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['고객예탁금'].pop(-1)\n",
    "                    dictionary['신용잔고'].pop(-1)\n",
    "                    dictionary['주식형 펀드'].pop(-1)\n",
    "                    dictionary['혼합형 펀드'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['고객예탁금']))\n",
    "        #print(len(dictionary['신용잔고']))\n",
    "        #print(len(dictionary['주식형 펀드']))\n",
    "        #print(len(dictionary['혼합형 펀드']))\n",
    "        #print(len(dictionary['채권형 펀드']))\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "\n",
    "    def get_money_trend_date(self,until_date=real_today,type=1):\n",
    "        \n",
    "        url = 'http://finance.naver.com/sise/sise_deposit.nhn?&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('&')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\moneytrend.xlsx'\n",
    "\n",
    "    \n",
    "        if type == 1:\n",
    "            until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_today\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        else:\n",
    "            moneytrend_df = pd.read_sql(\"select Date from moneytrend order by Date desc limit 1\", engine)\n",
    "            moneytrend_df = str(moneytrend_df['Date'])\n",
    "            until_date = moneytrend_df[5:15]\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        #df = DataFrame(columns = ['고객예탁금', '신용잔고','주식형 펀드','혼합형 펀드','채권형 펀드'])\n",
    "\n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "    \n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'고객예탁금': [],'신용잔고': [],'주식형 펀드': [],'혼합형 펀드': [],'채권형 펀드': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['고객예탁금','신용잔고','주식형 펀드','혼합형 펀드','채권형 펀드']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,3,5,7,9]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','rate_down','rate_up']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                        #if date_ <=  '19-03-05' :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df\n",
    "                        date_list.append(date_)\n",
    "                    \n",
    "                    elif count in mask:\n",
    "                        temp = int((count-1)/2)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                \n",
    "       \n",
    "                    count += 1\n",
    "            \n",
    "            \n",
    "    def get_kpi200(self):\n",
    "        \n",
    "        url = 'https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\kpi200.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'KPI200': [],'거래량': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['KPI200','거래량']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','number_1']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int(count/3)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['KPI200']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['KPI200'].pop(-1)\n",
    "                    dictionary['거래량'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        #print(len(date_list))\n",
    "        #print(len(dictionary['개인']))\n",
    "        #print(len(dictionary['외국인']))\n",
    "        #print(len(dictionary['기관']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "       \n",
    "\n",
    "    def get_kpi200_date(self,until_date=real_yesterday,type=1):\n",
    "    \n",
    "        url = 'https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\kpi200.xlsx'\n",
    "\n",
    "        if type == 1:\n",
    "            until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            #year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        else:\n",
    "            kpi200_df = pd.read_sql(\"select Date from kpi200 order by Date desc limit 1\", engine)\n",
    "            kpi200_df = str(kpi200_df['Date'])\n",
    "            until_date = kpi200_df[5:15]\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            #year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'KPI200': [],'거래량': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['KPI200','거래량']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [1,3]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','number_1']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                        #if date_ <=  '19-03-05' :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int(count/3)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                        #print(dictionary[name_list[temp]])\n",
    "                    count += 1\n",
    "                    \n",
    "                    \n",
    "    def get_programtrend(self):\n",
    "    \n",
    "        url = 'https://finance.naver.com/sise/programDealTrendDay.nhn?bizdate=20200315&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\programtrend.xlsx'\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'차익': [],'비차익': [],'전체': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['차익','비차익','전체']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [3,6,9]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','rate_down','rate_up','rate_noc']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        date_list.append(date_)\n",
    "                        \n",
    "                      \n",
    "                    elif count in mask:\n",
    "                        temp = int((count/3)-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "        \n",
    "                    count += 1\n",
    "                if len(date_list) != len(dictionary['전체']):\n",
    "                    print(str(i)+ '번째 페이지에서 누락된 값 발생')\n",
    "                    print('누락된 데이터를 제거합니다')\n",
    "                    \n",
    "                    date_list.pop(-1)\n",
    "                    dictionary['차익'].pop(-1)\n",
    "                    dictionary['비차익'].pop(-1)\n",
    "                    #dictionary['전체'].pop(-1)\n",
    "                \n",
    "        # 개별 list 요소 갯수 파악 \n",
    "        print(len(date_list))\n",
    "        print(len(dictionary['차익']))\n",
    "        print(len(dictionary['비차익']))\n",
    "        print(len(dictionary['전체']))\n",
    "\n",
    "        print(str(i) + '번째 페이지 크롤링 완료')\n",
    "        df = pd.DataFrame(dictionary,index = date_list)\n",
    "        df = df.sort_index()\n",
    "        df = df[['차익','비차익','전체']]\n",
    "        df.to_excel(path, encoding='utf-8')\n",
    "        print(df)\n",
    "            \n",
    "    def get_program_trend_date(self,until_date=real_yesterday, type=1):\n",
    "\n",
    "        url = 'https://finance.naver.com/sise/programDealTrendDay.nhn?bizdate=20200315&sosok=&page='\n",
    "\n",
    "        source = urlopen(url).read()   # 지정한 페이지에서 코드 읽기\n",
    "        source = BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
    "\n",
    "        last = source.find('td',class_='pgRR').find('a')['href']\n",
    "        last = last.split('page')[1]\n",
    "        last = last.split('=')[1]\n",
    "        last = int(last)\n",
    "        print(last)\n",
    "\n",
    "        # 사용자의 PC내 폴더 주소를 입력하시면 됩니다.\n",
    "        path = 'd:\\\\programtrend.xlsx'\n",
    "\n",
    "        if type == 1:\n",
    "            until_date = input(\"날짜를 입력하세요 sample: '2019-01-10': \") or real_yesterday\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        else:\n",
    "            programtrend_df = pd.read_sql(\"select Date from programtrend order by Date desc limit 1\", engine)\n",
    "            programtrend_df = str(programtrend_df['Date'])\n",
    "            until_date = programtrend_df[5:15]\n",
    "\n",
    "            year = until_date.split('-')[0]\n",
    "            mm = until_date.split('-')[1]\n",
    "            dd = until_date.split('-')[2]\n",
    "            year=year[2:]\n",
    "            until_date = year+'-'+mm+'-'+dd\n",
    "    \n",
    "        # 날짜를 받을 리스트\n",
    "        date_list = []\n",
    "\n",
    "        # 값을 받을 사전\n",
    "        dictionary = {'차익': [],'비차익': [],'전체': []}\n",
    "\n",
    "        # dictionary key 인덱싱을 위한 리스트\n",
    "        name_list = ['차익','비차익','전체']\n",
    "\n",
    "\n",
    "        # count mask\n",
    "        mask = [3,6,9]\n",
    "    \n",
    "        for i in range(1,last+1):\n",
    "        \n",
    "            source = urlopen(url+ str(i)).read()\n",
    "            source = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            #tbody = source.find('div',{'id':'wrap'}).find('div',{'class':'box_type_m'})\n",
    "            #trs = tbody.find_all('tr')\n",
    "\n",
    "            body = source.find('body')\n",
    "            trs = body.find_all('tr')\n",
    "\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td',{'class':['date','rate_down','rate_up','rate_noc']})\n",
    "                count = 0\n",
    "    \n",
    "                for td in tds:\n",
    "                    if count == 0:\n",
    "                        date_ = td.text.strip().replace('.','-')\n",
    "                        if date_ <=  until_date :\n",
    "                            df = pd.DataFrame(dictionary,index = date_list)\n",
    "                            df = df.sort_index()\n",
    "                            df = df[['차익','비차익','전체']]\n",
    "                            df.to_excel(path, encoding='utf-8')\n",
    "                            return df   \n",
    "                        date_list.append(date_)\n",
    "                        #print(date_list)\n",
    "                    elif count in mask:\n",
    "                        temp = int((count/3)-1)\n",
    "                        dictionary[name_list[temp]].append(td.text.strip().replace(',',''))\n",
    "                    \n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KPI200</th>\n",
       "      <th>거래량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-23</th>\n",
       "      <td>256.11</td>\n",
       "      <td>49981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            KPI200    거래량\n",
       "2019-08-23  256.11  49981"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_kpi200_date(type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "302 1217 9238 01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
