{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ##  일봉,주봉,월봉 데이터 생성\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "def day_week_month_data(market='hrs', start_day = '2020-01-01',period ='month'):\n",
    "    if market=='kospi' or market=='kosdaq':\n",
    "        df = select_market(market,start_day)\n",
    "    else :\n",
    "        df = select_stock(market,start_day)\n",
    "    df['Date']=pd.to_datetime(df['Date'])\n",
    "    months = [g for n, g in df.groupby(pd.Grouper(key='Date',freq='M'))]  ##   월별\n",
    "    weeks = [g for n, g in df.groupby(pd.Grouper(key='Date',freq='W'))]  ##   주별\n",
    "    columns = ['Date','Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    rows = []    \n",
    "\n",
    "    if period == 'day':\n",
    "        \n",
    "        df=df[['Date','Open', 'High', 'Low','Close', 'Volume']]\n",
    "        df.columns=columns\n",
    "        #df = df.set_index(df['date'])\n",
    "        return df\n",
    "    \n",
    "    elif period == 'month':\n",
    "        period = months\n",
    "        \n",
    "    elif period == 'week':\n",
    "        period = weeks\n",
    "        \n",
    "    for i in range(len(period)):\n",
    "        rows.append(period[i].iloc[-1]['Date'])\n",
    "        rows.append(period[i].iloc[0][\"Open\"])\n",
    "        rows.append(max(period[i]['High']))\n",
    "        rows.append(min(period[i]['Low']))\n",
    "        rows.append(period[i].iloc[-1]['Close'])\n",
    "        rows.append(sum(period[i]['Volume']))\n",
    "        \n",
    "    arr = np.array(rows)\n",
    "    arr1 = arr.reshape(len(period),6)\n",
    "    df = pd.DataFrame(data=arr1, columns=columns)\n",
    "    df = df.set_index(df['Date'])\n",
    "    df.rename(columns = {'Date' : 'Date1'}, inplace = True)\n",
    "    return df \n",
    "\n",
    "df = day_week_month_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Closed\"] = df[\"Close\"].shift(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff'] = df['Closed']/df['Close']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### DNN (Deep Neural Network)\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "#df = df[df.index_col=0]\n",
    "#df = df.values\n",
    "df=df[['Open','High','Low','Volume','Close']]\n",
    "print(df.shape)\n",
    "df = df.values\n",
    "df\n",
    "\n",
    "def split_xy5(dataset, time_steps, y_column):\n",
    "    #print(len(dataset))\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i+time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        #print('x:', x_end_number)\n",
    "        #print('y:', y_end_number)\n",
    "        if y_end_number > len(dataset):\n",
    "            \n",
    "            break\n",
    "        tmp_x = dataset[i:x_end_number, :]\n",
    "        tmp_y = dataset[x_end_number:y_end_number, 4]\n",
    "        \n",
    "        x.append(tmp_x)\n",
    "        y.append(tmp_y)\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "x, y = split_xy5(df, 5,1)\n",
    "\"\"\"print(x.shape)\n",
    "print(y.shape)\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size=0.3)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1]*x_test.shape[2]))\n",
    "\n",
    "\"\"\"\"print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "print('y_test:',y_test.shape)\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "#print(x_train_scaled[0, :])\n",
    "\n",
    "x_train_scaled = np.reshape(x_train_scaled,\n",
    "    (x_train_scaled.shape[0], 5, 5))\n",
    "x_test_scaled = np.reshape(x_test_scaled,\n",
    "    (x_test_scaled.shape[0], 5, 5))\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# 모델구성\n",
    "# 모델구성\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(5, 5)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=20)\n",
    "model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=1,\n",
    "          batch_size=1, epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\n",
    "print('loss : ', loss)\n",
    "print('mse : ', mse)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN (Deep Neural Network)\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "#df = df[df.index_col=0]\n",
    "#df = df.values\n",
    "df=df[['Open','High','Low','Volume','Close']]\n",
    "print(df.shape)\n",
    "df = df.values\n",
    "df\n",
    "\n",
    "def split_xy5(dataset, time_steps, y_column):\n",
    "    #print(len(dataset))\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i+time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        #print('x:', x_end_number)\n",
    "        #print('y:', y_end_number)\n",
    "        if y_end_number > len(dataset):\n",
    "            \n",
    "            break\n",
    "        tmp_x = dataset[i:x_end_number, :]\n",
    "        tmp_y = dataset[x_end_number:y_end_number, 4]\n",
    "        \n",
    "        x.append(tmp_x)\n",
    "        y.append(tmp_y)\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "x, y = split_xy5(df, 5,1)\n",
    "\"\"\"print(x.shape)\n",
    "print(y.shape)\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size=0.3)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1]*x_test.shape[2]))\n",
    "\n",
    "\"\"\"\"print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "print('y_test:',y_test.shape)\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "#print(x_train_scaled[0, :])\n",
    "\n",
    "x_train = np.reshape(x_train, x_train.shape[0], x_train.shape[1], x_train.shape[2])\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 모델구성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(25, ),activation='relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=20)\n",
    "model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=0,\n",
    "          batch_size=1, epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\n",
    "print('loss : ', loss)\n",
    "print('mse : ', mse)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN (Deep Neural Network)\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "#df = df[df.index_col=0]\n",
    "#df = df.values\n",
    "df=df[['Open','High','Low','Volume','Close']]\n",
    "print(df.shape)\n",
    "df = df.values\n",
    "df\n",
    "\n",
    "def split_xy5(dataset, time_steps, y_column):\n",
    "    #print(len(dataset))\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i+time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        #print('x:', x_end_number)\n",
    "        #print('y:', y_end_number)\n",
    "        if y_end_number > len(dataset):\n",
    "            \n",
    "            break\n",
    "        tmp_x = dataset[i:x_end_number, :]\n",
    "        tmp_y = dataset[x_end_number:y_end_number, 4]\n",
    "        \n",
    "        x.append(tmp_x)\n",
    "        y.append(tmp_y)\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "x, y = split_xy5(df, 5,1)\n",
    "\"\"\"print(x.shape)\n",
    "print(y.shape)\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size=0.3)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1]*x_test.shape[2]))\n",
    "\n",
    "\"\"\"\"print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "print('y_test:',y_test.shape)\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "print(x_train_scaled[0, :])\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 모델구성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(25, )))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=20)\n",
    "model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=0,\n",
    "          batch_size=1, epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\n",
    "print('loss : ', loss)\n",
    "print('mse : ', mse)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### RNN (Recurrent Neural Network)\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "#df = df[df.index_col=0]\n",
    "#df = df.values\n",
    "df=df[['Open','High','Low','Volume','Close']]\n",
    "print(df.shape)\n",
    "df = df.values\n",
    "df\n",
    "\n",
    "def split_xy5(dataset, time_steps, y_column):\n",
    "    #print(len(dataset))\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i+time_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        #print('x:', x_end_number)\n",
    "        #print('y:', y_end_number)\n",
    "        if y_end_number > len(dataset):\n",
    "            \n",
    "            break\n",
    "        tmp_x = dataset[i:x_end_number, :]\n",
    "        tmp_y = dataset[x_end_number:y_end_number, 4]\n",
    "        \n",
    "        x.append(tmp_x)\n",
    "        y.append(tmp_y)\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "x, y = split_xy5(df, 5,1)\n",
    "\"\"\"print(x.shape)\n",
    "print(y.shape)\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size=0.3)\n",
    "\n",
    "x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1]*x_test.shape[2]))\n",
    "\n",
    "\"\"\"\"print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "print('y_test:',y_test.shape)\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "print(x_train_scaled[0, :])\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 모델구성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(25, )))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=20)\n",
    "model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=1,\n",
    "          batch_size=1, epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\n",
    "print('loss : ', loss)\n",
    "print('mse : ', mse)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_scaled)\n",
    "for i in range(5):\n",
    "    print('종가:',y_test[i], '/ 예측가:', y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod1 import *\n",
    "df = select_stock('hrs','2020-09-01')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\n",
    "y_train = np.array([6,7,8])\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "\n",
    "print('x_train.shape.{}', format(x_train.shape))\n",
    "print('y_train.shape.{}', format(y_train.shape))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM,GRU,Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(8, input_shape = (5,1), activation='relu')))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "#예측\n",
    "x_predict = np.array([[4,5,6,7,8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0],x_predict.shape[1],1)\n",
    "print(x_predict.shape)\n",
    "y_predict = model.predict(x_predict)\n",
    "print('예측값:', y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\n",
    "y_train = np.array([6,7,8])\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "\n",
    "print('x_train.shape.{}', format(x_train.shape))\n",
    "print('y_train.shape.{}', format(y_train.shape))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM,GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(8, input_shape = (5,1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "#예측\n",
    "x_predict = np.array([[4,5,6,7,8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0],x_predict.shape[1],1)\n",
    "print(x_predict.shape)\n",
    "y_predict = model.predict(x_predict)\n",
    "print('예측값:', y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\n",
    "y_train = np.array([6,7,8])\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "\n",
    "print('x_train.shape.{}', format(x_train.shape))\n",
    "print('y_train.shape.{}', format(y_train.shape))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(8, input_shape = (5,1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=1)\n",
    "\n",
    "#예측\n",
    "x_predict = np.array([[4,5,6,7,8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0],x_predict.shape[1],1)\n",
    "print(x_predict.shape)\n",
    "y_predict = model.predict(x_predict)\n",
    "print('예측값:', y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\n",
    "y_train = np.array([6,7,8])\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "\n",
    "print('x_train.shape.{}', format(x_train.shape))\n",
    "print('y_train.shape.{}', format(y_train.shape))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(7, input_shape = (5,1), activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(x_train, y_train, epochs=300, batch_size=1)\n",
    "\n",
    "#예측\n",
    "x_predict = np.array([[4,5,6,7,8]])\n",
    "print(x_predict.shape)\n",
    "x_predict = x_predict.reshape(x_predict.shape[0],x_predict.shape[1],1)\n",
    "print(x_predict.shape)\n",
    "y_predict = model.predict(x_predict)\n",
    "print('예측값:', y_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, Y_train),(X_test,Y_test) = reuters.load_data(num_words=1000, test_split=0.2)\n",
    "\n",
    "category = np.max(Y_train)+1\n",
    "print(category, '카테고리')\n",
    "print(len(X_train),'학습용 뉴스기사')\n",
    "print(len(X_test), '테스트 뉴스기사')\n",
    "print(X_train[0])\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=100)\n",
    "y_train = np_utils.to_categorical(Y_train)\n",
    "y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,100))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "\n",
    "#  학습\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(x_train,y_train, batch_size=100, epochs=40, validation_data=(x_test,y_test))\n",
    "\n",
    "# 검증\n",
    "\n",
    "print('\\n Test Accuracy: %.4f' %(model.evaluate(x_test,y_test)[1]))\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker= '.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label = 'Trainset_losss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from keras import models, layers\n",
    "from keraspp import skeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod1 import *\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h_o'] = df['High']-df['Open']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h_l'] = df['High']-df['Low']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h_c'] = df['High']-df['Close']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Date','h_o','h_l','h_c','Volume']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 캔틀 차트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  mplfinance  라이브러리을 이용한 캔들차트 :  한글이 지원안됨....\n",
    "\n",
    "from mod1 import *\n",
    "#import matplotlib.gridspec as gridspec\n",
    "#import mplfinance as mpf\n",
    "\n",
    "\n",
    "df = select_stock_period('gh신소재', '2020-01-01')\n",
    "\n",
    "ohlc = df[['Date','Open', 'High', 'Low', 'Close','Volume']]\n",
    "ohlc['Date'] = pd.to_datetime(ohlc['Date'])\n",
    "ohlc = ohlc.set_index('Date')\n",
    "ohlc.index.name = 'Date'\n",
    "\n",
    "mpf.plot(ohlc,figratio=(15,8),figscale=1.0,type='candle',style='charles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod1 import *\n",
    "import matplotlib.gridspec as gridspec\n",
    "import mplfinance as mpf\n",
    "\n",
    "\n",
    "df = select_stock_period('gh신소재', '2020-01-01')\n",
    "\n",
    "ohlc = df[['Date','Open', 'High', 'Low', 'Close','Volume']]\n",
    "ohlc['Date'] = pd.to_datetime(ohlc['Date'])\n",
    "ohlc = ohlc.set_index('Date')\n",
    "ohlc.index.name = 'Date'\n",
    "\n",
    "plt.tight_layout()\n",
    "mpf.plot(ohlc,figratio=(16,4),type='candle',mav=(3,6,9),volume=True)  ### 3,6,9일 이동평균선 그리고 거래량\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  plotly 라이브러리를 이용한 캔들차트 , 한글지원됨\n",
    "\n",
    "from mod1 import *\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "df = select_stock_period('gh신소재' ,'2018-01-01')\n",
    "olhc = df[['Date','Open','High','Low','Close']]\n",
    "\n",
    "offline.init_notebook_mode(connected = True)\n",
    "\n",
    "trace = go.Candlestick(x=olhc.Date, open=olhc.Open, high=olhc.High, low=olhc.Low, close = olhc.Close)\n",
    "data =[trace]\n",
    "\n",
    "layout=go.Layout(title='gh신소재')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig,filename='candlestick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh 챠트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  stock 자료 항목을 월봉, 주봉  DataFrame 으로 변환하여 Bokeh Chart그리기\n",
    "\n",
    "from mod1 import *\n",
    "def group_candle(market='kospi', start_day = '2019-01-01',period ='month'):\n",
    "    df = select_market(market,start_day)\n",
    "    df['Date']=pd.to_datetime(df['Date'])\n",
    "    months = [g for n, g in df.groupby(pd.Grouper(key='Date',freq='M'))]  ##   월별\n",
    "    weeks = [g for n, g in df.groupby(pd.Grouper(key='Date',freq='W'))]  ##   주별\n",
    "    columns = ['date','open', 'high', 'low', 'close', 'volume']\n",
    "    rows = []\n",
    "    \n",
    "    if period == 'month':\n",
    "        period = months\n",
    "    elif period == 'week':\n",
    "        period = weeks\n",
    "        \n",
    "    for i in range(len(period)):\n",
    "        rows.append(period[i].iloc[-1]['Date'])\n",
    "        rows.append(period[i].iloc[0][\"Open\"])\n",
    "        rows.append(max(period[i]['High']))\n",
    "        rows.append(min(period[i]['Low']))\n",
    "        rows.append(period[i].iloc[-1]['Close'])\n",
    "        rows.append(sum(period[i]['Volume']))\n",
    "        \n",
    "    arr = np.array(rows)\n",
    "    arr1 = arr.reshape(len(period),6)\n",
    "    df = pd.DataFrame(data=arr1, columns=columns)\n",
    "    df = df.set_index(df['date'])\n",
    "    return df \n",
    "\n",
    "\n",
    "def bokeh_chart(market='kospi',start_day = '2019-01-01', period ='month'):\n",
    "    from math import pi\n",
    "    from bokeh.io import output_notebook, show\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.layouts import gridplot\n",
    "\n",
    "    output_notebook()\n",
    "    \n",
    "    df = group_candle(market, start_day, period)\n",
    "    \n",
    "    mids = (df.open + df.close)/2\n",
    "    spans = abs(df.close-df.open)\n",
    "\n",
    "    inc = df.close >= df.open\n",
    "    dec = df.open > df.close\n",
    "\n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,crosshair\"\n",
    "\n",
    "    p_candlechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=900, plot_height=200, toolbar_location=\"left\",title = market)\n",
    "    p_candlechart.xaxis.major_label_orientation = pi/4\n",
    "    p_candlechart.segment(df.index[inc], df.high[inc], df.index[inc], df.low[inc], color=\"red\")\n",
    "    p_candlechart.segment(df.index[dec], df.high[dec], df.index[dec], df.low[dec], color=\"blue\")\n",
    "    p_candlechart.vbar(df.index[inc], 0.5, df.open[inc], df.close[inc], fill_color=\"red\", line_color=\"red\",line_width=10)\n",
    "    p_candlechart.vbar(df.index[dec], 0.5, df.open[dec], df.close[dec], fill_color=\"blue\", line_color=\"blue\",line_width=10)\n",
    "\n",
    "    p_volumechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=900, plot_height=200, toolbar_location=\"left\")\n",
    "    p_volumechart.vbar(df.index, 0.5, df.volume, fill_color=\"black\", line_color=\"black\",line_width=10)\n",
    "\n",
    "    p = figure(tools='crosshair', plot_width=900, toolbar_location=\"left\")\n",
    "    p = gridplot([[p_candlechart], [p_volumechart]], toolbar_location='left')\n",
    "    show(p)\n",
    "\n",
    "bokeh_chart('kospi','2020-01-01','week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod1 import *\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def datetime(x):\n",
    "    return np.array(x, dtype=np.datetime64)\n",
    "\n",
    "df = get_index_ohlcv_by_date(\"20190101\", \"20200528\", \"코스피\", freq='m')\n",
    "df = df.reset_index()\n",
    "df.columns=['date', 'open', 'high', 'low','close','volume']\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "p1 = figure(x_axis_type=\"datetime\", title=\"Stock Closing Prices\")\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.xaxis.axis_label = 'Date'\n",
    "p1.yaxis.axis_label = 'Price'\n",
    "\n",
    "p1.line(datetime(df['date']), df['close'], color='#A6CEE3', legend_label='HRS')\n",
    "p1.legend.location = \"top_left\"\n",
    "\n",
    "hrs = np.array(df['close'])\n",
    "hrs_dates = np.array(df['date'], dtype=np.datetime64)\n",
    "\n",
    "window_size = 30\n",
    "window = np.ones(window_size)/float(window_size)\n",
    "hrs_avg = np.convolve(hrs, window, 'same')\n",
    "\n",
    "p2 = figure(x_axis_type=\"datetime\", title=\"HRS One-Month Average\")\n",
    "p2.grid.grid_line_alpha = 0\n",
    "p2.xaxis.axis_label = 'Date'\n",
    "p2.yaxis.axis_label = 'Price'\n",
    "p2.ygrid.band_fill_color = \"olive\"\n",
    "p2.ygrid.band_fill_alpha = 0.1\n",
    "\n",
    "p2.circle(hrs_dates, hrs, size=4, legend_label='close',\n",
    "          color='red', alpha=0.2)\n",
    "\n",
    "p2.line(hrs_dates, hrs_avg, legend_label='avg', color='navy')\n",
    "p2.legend.location = \"top_left\"\n",
    "\n",
    "#output_file(\"stocks.html\", title=\"stocks.py example\")\n",
    "\n",
    "show(gridplot([[p1,p2]], plot_width=400, plot_height=400))  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  코스피 월봉(함수)\n",
    "\n",
    "from mod1 import *\n",
    "\n",
    "def bokeh_chart(start_day='20190101', last_day='20210528',market='코스피', freq='m' ):\n",
    "    from math import pi\n",
    "    from bokeh.io import output_notebook, show\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.layouts import gridplot\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    df = get_index_ohlcv_by_date(\"20190101\", \"20200528\", \"코스피\", freq='m')\n",
    "    df = df.reset_index()\n",
    "    df.columns=['date', 'open', 'high', 'low','close','volume']\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.set_index(df['date'])\n",
    "\n",
    "    mids = (df.open + df.close)/2\n",
    "    spans = abs(df.close-df.open)\n",
    "\n",
    "    inc = df.close >= df.open\n",
    "    dec = df.open > df.close\n",
    "\n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,crosshair\"\n",
    "\n",
    "    p_candlechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=900, plot_height=200, toolbar_location=\"left\")\n",
    "    p_candlechart.xaxis.major_label_orientation = pi/4\n",
    "    p_candlechart.segment(df.index[inc], df.high[inc], df.index[inc], df.low[inc], color=\"red\")\n",
    "    p_candlechart.segment(df.index[dec], df.high[dec], df.index[dec], df.low[dec], color=\"blue\")\n",
    "    p_candlechart.vbar(df.index[inc], 0.5, df.open[inc], df.close[inc], fill_color=\"red\", line_color=\"red\",line_width=30)\n",
    "    p_candlechart.vbar(df.index[dec], 0.5, df.open[dec], df.close[dec], fill_color=\"blue\", line_color=\"blue\",line_width=30)\n",
    "\n",
    "    p_volumechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=900, plot_height=200, toolbar_location=\"left\")\n",
    "    p_volumechart.vbar(df.index, 0.5, df.volume, fill_color=\"black\", line_color=\"black\",line_width=30)\n",
    "\n",
    "    p = figure(tools='crosshair', plot_width=900, toolbar_location=\"left\")\n",
    "    p = gridplot([[p_candlechart], [p_volumechart]], toolbar_location='left')\n",
    "    show(p)\n",
    "bokeh_chart(market='코스닥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  코스피 일봉\n",
    "\n",
    "from mod1 import *\n",
    "from math import pi\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "df = select_market('kospi','2020-01-01')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "df.columns=['date', 'open', 'high', 'low', 'volume','close']\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "mids = (df.open + df.close)/2\n",
    "spans = abs(df.close-df.open)\n",
    "\n",
    "inc = df.close >= df.open\n",
    "dec = df.open > df.close\n",
    "w = 12*60*60*1000 # half day in ms\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,crosshair\"\n",
    "\n",
    "p_candlechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=1050, plot_height=200, toolbar_location=\"left\")\n",
    "p_candlechart.xaxis.major_label_orientation = pi/4\n",
    "\n",
    "p_candlechart.segment(df.date, df.high, df.date, df.low, color=\"black\")\n",
    "p_candlechart.rect(df.date[inc], mids[inc], w, spans[inc], fill_color=\"#D5E1DD\", line_color=\"blue\")\n",
    "p_candlechart.rect(df.date[dec], mids[dec], w, spans[dec], fill_color=\"#F2583E\", line_color=\"red\")\n",
    "\n",
    "p_volumechart = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=1050, plot_height=200, toolbar_location=\"left\")\n",
    "p_volumechart.vbar(df.date, 2.0, df.volume, fill_color=\"black\", line_color=\"black\",line_width=3)\n",
    "\n",
    "p = figure(tools='crosshair', plot_width=1000, toolbar_location=\"left\")\n",
    "p = gridplot([[p_candlechart], [p_volumechart]], toolbar_location='left')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###  개별종목 일봉\n",
    "\n",
    "from mod1 import *\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "df = select_stock('hrs','2018-01-01')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "df.columns=['date', 'open', 'high', 'low', 'volume','close']\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "inc = df.close >= df.open\n",
    "dec = df.open > df.close\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,crosshair\"\n",
    "\n",
    "p_candlechart = figure(x_axis_type=\"datetime\",plot_width=1050, plot_height=200, x_range=(-1, len(df)),tools=TOOLS, toolbar_location=\"left\")\n",
    "p_candlechart.segment(df.index[inc], df.high[inc], df.index[inc], df.low[inc], color=\"red\")\n",
    "p_candlechart.segment(df.index[dec], df.high[dec], df.index[dec], df.low[dec], color=\"blue\")\n",
    "p_candlechart.vbar(df.index[inc], 0.5, df.open[inc], df.close[inc], fill_color=\"red\", line_color=\"red\")\n",
    "p_candlechart.vbar(df.index[dec], 0.5, df.open[dec], df.close[dec], fill_color=\"blue\", line_color=\"blue\")\n",
    "\n",
    "p_volumechart = figure(plot_width=1050, plot_height=100, x_range=p_candlechart.x_range, tools=\"crosshair\",toolbar_location=\"below\")\n",
    "p_volumechart.vbar(df.index, 0.5, df.volume, fill_color=\"black\", line_color=\"black\")\n",
    "\n",
    "p = figure(tools='crosshair', plot_width=1000, toolbar_location=\"left\")\n",
    "p = gridplot([[p_candlechart], [p_volumechart]], toolbar_location='left')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "df = select_stock('hrs','2018-01-01')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "df.columns=['date', 'open', 'high', 'low', 'volume','close']\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "mids = (df.open + df.close)/2\n",
    "spans = abs(df.close-df.open)\n",
    "\n",
    "inc = df.close > df.open\n",
    "dec = df.open > df.close\n",
    "w = 12*60*60*1000 # half day in ms\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,crosshair\"\n",
    "\n",
    "p = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=1000, toolbar_location=\"left\")\n",
    "\n",
    "#p.title = \"MSFT Candlestick\"\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.grid.grid_line_alpha=0.3\n",
    "\n",
    "p.segment(df.date, df.high, df.date, df.low, color=\"black\")\n",
    "p.rect(df.date[inc], mids[inc], w, spans[inc], fill_color=\"#D5E1DD\", line_color=\"blue\")\n",
    "p.rect(df.date[dec], mids[dec], w, spans[dec], fill_color=\"#F2583E\", line_color=\"red\")\n",
    "\n",
    "#output_file(\"candlestick.html\", title=\"candlestick.py example\")\n",
    "#p = gridplot([[p_candlechart], [p_volumechart]], toolbar_location=None)\n",
    "#p = gridplot([[p_candlechart]],toolbar_location=None)\n",
    "show(p)  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod1 import *\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "def datetime(x):\n",
    "    return np.array(x, dtype=np.datetime64)\n",
    "\n",
    "df = select_stock('hrs','2020-01-01')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "df.columns=['date', 'open', 'high', 'low', 'volume','close']\n",
    "\n",
    "p1 = figure(x_axis_type=\"datetime\", title=\"Stock Closing Prices\")\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.xaxis.axis_label = 'Date'\n",
    "p1.yaxis.axis_label = 'Price'\n",
    "\n",
    "p1.line(datetime(df['date']), df['close'], color='#A6CEE3', legend_label='HRS')\n",
    "p1.legend.location = \"top_left\"\n",
    "\n",
    "hrs = np.array(df['close'])\n",
    "hrs_dates = np.array(df['date'], dtype=np.datetime64)\n",
    "\n",
    "window_size = 30\n",
    "window = np.ones(window_size)/float(window_size)\n",
    "hrs_avg = np.convolve(hrs, window, 'same')\n",
    "\n",
    "p2 = figure(x_axis_type=\"datetime\", title=\"HRS One-Month Average\")\n",
    "p2.grid.grid_line_alpha = 0\n",
    "p2.xaxis.axis_label = 'Date'\n",
    "p2.yaxis.axis_label = 'Price'\n",
    "p2.ygrid.band_fill_color = \"olive\"\n",
    "p2.ygrid.band_fill_alpha = 0.1\n",
    "\n",
    "p2.circle(hrs_dates, hrs, size=4, legend_label='close',\n",
    "          color='red', alpha=0.2)\n",
    "\n",
    "p2.line(hrs_dates, hrs_avg, legend_label='avg', color='navy')\n",
    "p2.legend.location = \"top_left\"\n",
    "\n",
    "#output_file(\"stocks.html\", title=\"stocks.py example\")\n",
    "\n",
    "show(gridplot([[p1,p2]], plot_width=400, plot_height=400))  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "def datetime(x):\n",
    "    return np.array(x, dtype=np.datetime64)\n",
    "\n",
    "name = '오공'\n",
    "name1 = name\n",
    "\n",
    "df = select_stock(name,'2019-01-01')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "df.columns=['date', 'open', 'high', 'low', 'volume','close']\n",
    "\n",
    "p1 = figure(x_axis_type=\"datetime\", title=\"Stock Closing Prices\")\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.xaxis.axis_label = 'Date'\n",
    "p1.yaxis.axis_label = 'Price'\n",
    "\n",
    "p1.line(datetime(df['date']), df['close'], color='#A6CEE3', legend_label=name)\n",
    "p1.legend.location = \"top_left\"\n",
    "\n",
    "name = np.array(df['close'])\n",
    "name1_dates = np.array(df['date'], dtype=np.datetime64)\n",
    "\n",
    "window_size = 30\n",
    "window = np.ones(window_size)/float(window_size)\n",
    "name1_avg = np.convolve(name, window, 'same')\n",
    "\n",
    "p2 = figure(x_axis_type=\"datetime\", title= name1+\" One-Month Average\")\n",
    "p2.grid.grid_line_alpha = 0\n",
    "p2.xaxis.axis_label = 'Date'\n",
    "p2.yaxis.axis_label = 'Price'\n",
    "p2.ygrid.band_fill_color = \"olive\"\n",
    "p2.ygrid.band_fill_alpha = 0.1\n",
    "\n",
    "p2.circle(name1_dates, name, size=4, legend_label='close',\n",
    "          color='darkgrey', alpha=0.2)\n",
    "\n",
    "p2.line(name1_dates, name1_avg, legend_label='avg', color='navy')\n",
    "p2.legend.location = \"top_right\"\n",
    "\n",
    "#output_file(\"stocks.html\", title=\"stocks.py example\")\n",
    "\n",
    "show(gridplot([[p1,p2]], plot_width=400, plot_height=400))  # open a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical-Analysis without Talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.quantopian.com/posts/technical-analysis-indicators-without-talib-code\n",
    "\n",
    "\n",
    "import numpy  \n",
    "import pandas as pd  \n",
    "import math as m\n",
    "\n",
    "\n",
    "#Moving Average  \n",
    "def MA(df, n):  \n",
    "    MA = pd.Series(pd.rolling_mean(df['Close'], n), name = 'MA_' + str(n))  \n",
    "    df = df.join(MA)  \n",
    "    return df\n",
    "\n",
    "#Exponential Moving Average  \n",
    "def EMA(df, n):  \n",
    "    EMA = pd.Series(pd.ewma(df['Close'], span = n, min_periods = n - 1), name = 'EMA_' + str(n))  \n",
    "    df = df.join(EMA)  \n",
    "    return df\n",
    "\n",
    "#Momentum  \n",
    "def MOM(df, n):  \n",
    "    M = pd.Series(df['Close'].diff(n), name = 'Momentum_' + str(n))  \n",
    "    df = df.join(M)  \n",
    "    return df\n",
    "\n",
    "#Rate of Change  \n",
    "def ROC(df, n):  \n",
    "    M = df['Close'].diff(n - 1)  \n",
    "    N = df['Close'].shift(n - 1)  \n",
    "    ROC = pd.Series(M / N, name = 'ROC_' + str(n))  \n",
    "    df = df.join(ROC)  \n",
    "    return df\n",
    "\n",
    "#Average True Range  \n",
    "def ATR(df,n): #df is the DataFrame, n is the period 7,14 ,etc\n",
    "    df['H-L']=abs(df['High']-df['Low'])\n",
    "    df['H-PC']=abs(df['High']-df['Close'].shift(1))\n",
    "    df['L-PC']=abs(df['Low']-df['Close'].shift(1))\n",
    "    df['TR']=df[['H-L','H-PC','L-PC']].max(axis=1)\n",
    "    df['ATR']=np.nan\n",
    "    df.at[n-1,'ATR']=df['TR'][:n-1].mean() #.ix is deprecated from pandas version- 0.19\n",
    "    for i in range(n,len(df)):\n",
    "        df['ATR'][i]=(df['ATR'][i-1]*(n-1)+ df['TR'][i])/n\n",
    "    return df\n",
    "\n",
    "#Bollinger Bands  \n",
    "def BBANDS(df, n):  \n",
    "    MA = pd.Series(pd.rolling_mean(df['Close'], n))  \n",
    "    MSD = pd.Series(pd.rolling_std(df['Close'], n))  \n",
    "    b1 = 4 * MSD / MA  \n",
    "    B1 = pd.Series(b1, name = 'BollingerB_' + str(n))  \n",
    "    df = df.join(B1)  \n",
    "    b2 = (df['Close'] - MA + 2 * MSD) / (4 * MSD)  \n",
    "    B2 = pd.Series(b2, name = 'Bollinger%b_' + str(n))  \n",
    "    df = df.join(B2)  \n",
    "    return df\n",
    "\n",
    "#Pivot Points, Supports and Resistances  \n",
    "def PPSR(df):  \n",
    "    PP = pd.Series((df['High'] + df['Low'] + df['Close']) / 3)  \n",
    "    R1 = pd.Series(2 * PP - df['Low'])  \n",
    "    S1 = pd.Series(2 * PP - df['High'])  \n",
    "    R2 = pd.Series(PP + df['High'] - df['Low'])  \n",
    "    S2 = pd.Series(PP - df['High'] + df['Low'])  \n",
    "    R3 = pd.Series(df['High'] + 2 * (PP - df['Low']))  \n",
    "    S3 = pd.Series(df['Low'] - 2 * (df['High'] - PP))  \n",
    "    psr = {'PP':PP, 'R1':R1, 'S1':S1, 'R2':R2, 'S2':S2, 'R3':R3, 'S3':S3}  \n",
    "    PSR = pd.DataFrame(psr)  \n",
    "    df = df.join(PSR)  \n",
    "    return df\n",
    "\n",
    "#Stochastic oscillator %K  \n",
    "def STOK(df):  \n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name = 'SO%k')  \n",
    "    df = df.join(SOk)  \n",
    "    return df\n",
    "\n",
    "# Stochastic Oscillator, EMA smoothing, nS = slowing (1 if no slowing)  \n",
    "def STO(df,  nK, nD, nS=1):  \n",
    "    SOk = pd.Series((df['Close'] - df['Low'].rolling(nK).min()) / (df['High'].rolling(nK).max() - df['Low'].rolling(nK).min()), name = 'SO%k'+str(nK))  \n",
    "    SOd = pd.Series(SOk.ewm(ignore_na=False, span=nD, min_periods=nD-1, adjust=True).mean(), name = 'SO%d'+str(nD))  \n",
    "    SOk = SOk.ewm(ignore_na=False, span=nS, min_periods=nS-1, adjust=True).mean()  \n",
    "    SOd = SOd.ewm(ignore_na=False, span=nS, min_periods=nS-1, adjust=True).mean()  \n",
    "    df = df.join(SOk)  \n",
    "    df = df.join(SOd)  \n",
    "    return df  \n",
    "# Stochastic Oscillator, SMA smoothing, nS = slowing (1 if no slowing)  \n",
    "def STO(df, nK, nD,  nS=1):  \n",
    "    SOk = pd.Series((df['Close'] - df['Low'].rolling(nK).min()) / (df['High'].rolling(nK).max() - df['Low'].rolling(nK).min()), name = 'SO%k'+str(nK))  \n",
    "    SOd = pd.Series(SOk.rolling(window=nD, center=False).mean(), name = 'SO%d'+str(nD))  \n",
    "    SOk = SOk.rolling(window=nS, center=False).mean()  \n",
    "    SOd = SOd.rolling(window=nS, center=False).mean()  \n",
    "    df = df.join(SOk)  \n",
    "    df = df.join(SOd)  \n",
    "    return df  \n",
    "#Trix  \n",
    "def TRIX(df, n):  \n",
    "    EX1 = pd.ewma(df['Close'], span = n, min_periods = n - 1)  \n",
    "    EX2 = pd.ewma(EX1, span = n, min_periods = n - 1)  \n",
    "    EX3 = pd.ewma(EX2, span = n, min_periods = n - 1)  \n",
    "    i = 0  \n",
    "    ROC_l = [0]  \n",
    "    while i + 1 <= df.index[-1]:  \n",
    "        ROC = (EX3[i + 1] - EX3[i]) / EX3[i]  \n",
    "        ROC_l.append(ROC)  \n",
    "        i = i + 1  \n",
    "    Trix = pd.Series(ROC_l, name = 'Trix_' + str(n))  \n",
    "    df = df.join(Trix)  \n",
    "    return df\n",
    "\n",
    "#Average Directional Movement Index  \n",
    "def ADX(df, n, n_ADX):  \n",
    "    i = 0  \n",
    "    UpI = []  \n",
    "    DoI = []  \n",
    "    while i + 1 <= df.index[-1]:  \n",
    "        UpMove = df.get_value(i + 1, 'High') - df.get_value(i, 'High')  \n",
    "        DoMove = df.get_value(i, 'Low') - df.get_value(i + 1, 'Low')  \n",
    "        if UpMove > DoMove and UpMove > 0:  \n",
    "            UpD = UpMove  \n",
    "        else: UpD = 0  \n",
    "        UpI.append(UpD)  \n",
    "        if DoMove > UpMove and DoMove > 0:  \n",
    "            DoD = DoMove  \n",
    "        else: DoD = 0  \n",
    "        DoI.append(DoD)  \n",
    "        i = i + 1  \n",
    "    i = 0  \n",
    "    TR_l = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        TR = max(df.get_value(i + 1, 'High'), df.get_value(i, 'Close')) - min(df.get_value(i + 1, 'Low'), df.get_value(i, 'Close'))  \n",
    "        TR_l.append(TR)  \n",
    "        i = i + 1  \n",
    "    TR_s = pd.Series(TR_l)  \n",
    "    ATR = pd.Series(pd.ewma(TR_s, span = n, min_periods = n))  \n",
    "    UpI = pd.Series(UpI)  \n",
    "    DoI = pd.Series(DoI)  \n",
    "    PosDI = pd.Series(pd.ewma(UpI, span = n, min_periods = n - 1) / ATR)  \n",
    "    NegDI = pd.Series(pd.ewma(DoI, span = n, min_periods = n - 1) / ATR)  \n",
    "    ADX = pd.Series(pd.ewma(abs(PosDI - NegDI) / (PosDI + NegDI), span = n_ADX, min_periods = n_ADX - 1), name = 'ADX_' + str(n) + '_' + str(n_ADX))  \n",
    "    df = df.join(ADX)  \n",
    "    return df\n",
    "\n",
    "#MACD, MACD Signal and MACD difference  \n",
    "def MACD(df, n_fast, n_slow):  \n",
    "    EMAfast = pd.Series(pd.ewma(df['Close'], span = n_fast, min_periods = n_slow - 1))  \n",
    "    EMAslow = pd.Series(pd.ewma(df['Close'], span = n_slow, min_periods = n_slow - 1))  \n",
    "    MACD = pd.Series(EMAfast - EMAslow, name = 'MACD_' + str(n_fast) + '_' + str(n_slow))  \n",
    "    MACDsign = pd.Series(pd.ewma(MACD, span = 9, min_periods = 8), name = 'MACDsign_' + str(n_fast) + '_' + str(n_slow))  \n",
    "    MACDdiff = pd.Series(MACD - MACDsign, name = 'MACDdiff_' + str(n_fast) + '_' + str(n_slow))  \n",
    "    df = df.join(MACD)  \n",
    "    df = df.join(MACDsign)  \n",
    "    df = df.join(MACDdiff)  \n",
    "    return df\n",
    "\n",
    "#Mass Index  \n",
    "def MassI(df):  \n",
    "    Range = df['High'] - df['Low']  \n",
    "    EX1 = pd.ewma(Range, span = 9, min_periods = 8)  \n",
    "    EX2 = pd.ewma(EX1, span = 9, min_periods = 8)  \n",
    "    Mass = EX1 / EX2  \n",
    "    MassI = pd.Series(pd.rolling_sum(Mass, 25), name = 'Mass Index')  \n",
    "    df = df.join(MassI)  \n",
    "    return df\n",
    "\n",
    "#Vortex Indicator: http://www.vortexindicator.com/VFX_VORTEX.PDF  \n",
    "def Vortex(df, n):  \n",
    "    i = 0  \n",
    "    TR = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        Range = max(df.get_value(i + 1, 'High'), df.get_value(i, 'Close')) - min(df.get_value(i + 1, 'Low'), df.get_value(i, 'Close'))  \n",
    "        TR.append(Range)  \n",
    "        i = i + 1  \n",
    "    i = 0  \n",
    "    VM = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        Range = abs(df.get_value(i + 1, 'High') - df.get_value(i, 'Low')) - abs(df.get_value(i + 1, 'Low') - df.get_value(i, 'High'))  \n",
    "        VM.append(Range)  \n",
    "        i = i + 1  \n",
    "    VI = pd.Series(pd.rolling_sum(pd.Series(VM), n) / pd.rolling_sum(pd.Series(TR), n), name = 'Vortex_' + str(n))  \n",
    "    df = df.join(VI)  \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#KST Oscillator  \n",
    "def KST(df, r1, r2, r3, r4, n1, n2, n3, n4):  \n",
    "    M = df['Close'].diff(r1 - 1)  \n",
    "    N = df['Close'].shift(r1 - 1)  \n",
    "    ROC1 = M / N  \n",
    "    M = df['Close'].diff(r2 - 1)  \n",
    "    N = df['Close'].shift(r2 - 1)  \n",
    "    ROC2 = M / N  \n",
    "    M = df['Close'].diff(r3 - 1)  \n",
    "    N = df['Close'].shift(r3 - 1)  \n",
    "    ROC3 = M / N  \n",
    "    M = df['Close'].diff(r4 - 1)  \n",
    "    N = df['Close'].shift(r4 - 1)  \n",
    "    ROC4 = M / N  \n",
    "    KST = pd.Series(pd.rolling_sum(ROC1, n1) + pd.rolling_sum(ROC2, n2) * 2 + pd.rolling_sum(ROC3, n3) * 3 + pd.rolling_sum(ROC4, n4) * 4, name = 'KST_' + str(r1) + '_' + str(r2) + '_' + str(r3) + '_' + str(r4) + '_' + str(n1) + '_' + str(n2) + '_' + str(n3) + '_' + str(n4))  \n",
    "    df = df.join(KST)  \n",
    "    return df\n",
    "\n",
    "#Relative Strength Index  \n",
    "def RSI(df, n):  \n",
    "    i = 0  \n",
    "    UpI = [0]  \n",
    "    DoI = [0]  \n",
    "    while i + 1 <= df.index[-1]:  \n",
    "        UpMove = df.get_value(i + 1, 'High') - df.get_value(i, 'High')  \n",
    "        DoMove = df.get_value(i, 'Low') - df.get_value(i + 1, 'Low')  \n",
    "        if UpMove > DoMove and UpMove > 0:  \n",
    "            UpD = UpMove  \n",
    "        else: UpD = 0  \n",
    "        UpI.append(UpD)  \n",
    "        if DoMove > UpMove and DoMove > 0:  \n",
    "            DoD = DoMove  \n",
    "        else: DoD = 0  \n",
    "        DoI.append(DoD)  \n",
    "        i = i + 1  \n",
    "    UpI = pd.Series(UpI)  \n",
    "    DoI = pd.Series(DoI)  \n",
    "    PosDI = pd.Series(pd.ewma(UpI, span = n, min_periods = n - 1))  \n",
    "    NegDI = pd.Series(pd.ewma(DoI, span = n, min_periods = n - 1))  \n",
    "    RSI = pd.Series(PosDI / (PosDI + NegDI), name = 'RSI_' + str(n))  \n",
    "    df = df.join(RSI)  \n",
    "    return df\n",
    "\n",
    "#True Strength Index  \n",
    "def TSI(df, r, s):  \n",
    "    M = pd.Series(df['Close'].diff(1))  \n",
    "    aM = abs(M)  \n",
    "    EMA1 = pd.Series(pd.ewma(M, span = r, min_periods = r - 1))  \n",
    "    aEMA1 = pd.Series(pd.ewma(aM, span = r, min_periods = r - 1))  \n",
    "    EMA2 = pd.Series(pd.ewma(EMA1, span = s, min_periods = s - 1))  \n",
    "    aEMA2 = pd.Series(pd.ewma(aEMA1, span = s, min_periods = s - 1))  \n",
    "    TSI = pd.Series(EMA2 / aEMA2, name = 'TSI_' + str(r) + '_' + str(s))  \n",
    "    df = df.join(TSI)  \n",
    "    return df\n",
    "\n",
    "#Accumulation/Distribution  \n",
    "def ACCDIST(df, n):  \n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']  \n",
    "    M = ad.diff(n - 1)  \n",
    "    N = ad.shift(n - 1)  \n",
    "    ROC = M / N  \n",
    "    AD = pd.Series(ROC, name = 'Acc/Dist_ROC_' + str(n))  \n",
    "    df = df.join(AD)  \n",
    "    return df\n",
    "\n",
    "#Chaikin Oscillator  \n",
    "def Chaikin(df):  \n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']  \n",
    "    Chaikin = pd.Series(pd.ewma(ad, span = 3, min_periods = 2) - pd.ewma(ad, span = 10, min_periods = 9), name = 'Chaikin')  \n",
    "    df = df.join(Chaikin)  \n",
    "    return df\n",
    "\n",
    "#Money Flow Index and Ratio  \n",
    "def MFI(df, n):  \n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3  \n",
    "    i = 0  \n",
    "    PosMF = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        if PP[i + 1] > PP[i]:  \n",
    "            PosMF.append(PP[i + 1] * df.get_value(i + 1, 'Volume'))  \n",
    "        else:  \n",
    "            PosMF.append(0)  \n",
    "        i = i + 1  \n",
    "    PosMF = pd.Series(PosMF)  \n",
    "    TotMF = PP * df['Volume']  \n",
    "    MFR = pd.Series(PosMF / TotMF)  \n",
    "    MFI = pd.Series(pd.rolling_mean(MFR, n), name = 'MFI_' + str(n))  \n",
    "    df = df.join(MFI)  \n",
    "    return df\n",
    "\n",
    "#On-balance Volume  \n",
    "def OBV(df, n):  \n",
    "    i = 0  \n",
    "    OBV = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        if df.get_value(i + 1, 'Close') - df.get_value(i, 'Close') > 0:  \n",
    "            OBV.append(df.get_value(i + 1, 'Volume'))  \n",
    "        if df.get_value(i + 1, 'Close') - df.get_value(i, 'Close') == 0:  \n",
    "            OBV.append(0)  \n",
    "        if df.get_value(i + 1, 'Close') - df.get_value(i, 'Close') < 0:  \n",
    "            OBV.append(-df.get_value(i + 1, 'Volume'))  \n",
    "        i = i + 1  \n",
    "    OBV = pd.Series(OBV)  \n",
    "    OBV_ma = pd.Series(pd.rolling_mean(OBV, n), name = 'OBV_' + str(n))  \n",
    "    df = df.join(OBV_ma)  \n",
    "    return df\n",
    "\n",
    "#Force Index  \n",
    "def FORCE(df, n):  \n",
    "    F = pd.Series(df['Close'].diff(n) * df['Volume'].diff(n), name = 'Force_' + str(n))  \n",
    "    df = df.join(F)  \n",
    "    return df\n",
    "\n",
    "#Ease of Movement  \n",
    "def EOM(df, n):  \n",
    "    EoM = (df['High'].diff(1) + df['Low'].diff(1)) * (df['High'] - df['Low']) / (2 * df['Volume'])  \n",
    "    Eom_ma = pd.Series(pd.rolling_mean(EoM, n), name = 'EoM_' + str(n))  \n",
    "    df = df.join(Eom_ma)  \n",
    "    return df\n",
    "\n",
    "#Commodity Channel Index  \n",
    "def CCI(df, n):  \n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3  \n",
    "    CCI = pd.Series((PP - pd.rolling_mean(PP, n)) / pd.rolling_std(PP, n), name = 'CCI_' + str(n))  \n",
    "    df = df.join(CCI)  \n",
    "    return df\n",
    "\n",
    "#Coppock Curve  \n",
    "def COPP(df, n):  \n",
    "    M = df['Close'].diff(int(n * 11 / 10) - 1)  \n",
    "    N = df['Close'].shift(int(n * 11 / 10) - 1)  \n",
    "    ROC1 = M / N  \n",
    "    M = df['Close'].diff(int(n * 14 / 10) - 1)  \n",
    "    N = df['Close'].shift(int(n * 14 / 10) - 1)  \n",
    "    ROC2 = M / N  \n",
    "    Copp = pd.Series(pd.ewma(ROC1 + ROC2, span = n, min_periods = n), name = 'Copp_' + str(n))  \n",
    "    df = df.join(Copp)  \n",
    "    return df\n",
    "\n",
    "#Keltner Channel  \n",
    "def KELCH(df, n):  \n",
    "    KelChM = pd.Series(pd.rolling_mean((df['High'] + df['Low'] + df['Close']) / 3, n), name = 'KelChM_' + str(n))  \n",
    "    KelChU = pd.Series(pd.rolling_mean((4 * df['High'] - 2 * df['Low'] + df['Close']) / 3, n), name = 'KelChU_' + str(n))  \n",
    "    KelChD = pd.Series(pd.rolling_mean((-2 * df['High'] + 4 * df['Low'] + df['Close']) / 3, n), name = 'KelChD_' + str(n))  \n",
    "    df = df.join(KelChM)  \n",
    "    df = df.join(KelChU)  \n",
    "    df = df.join(KelChD)  \n",
    "    return df\n",
    "\n",
    "#Ultimate Oscillator  \n",
    "def ULTOSC(df):  \n",
    "    i = 0  \n",
    "    TR_l = [0]  \n",
    "    BP_l = [0]  \n",
    "    while i < df.index[-1]:  \n",
    "        TR = max(df.get_value(i + 1, 'High'), df.get_value(i, 'Close')) - min(df.get_value(i + 1, 'Low'), df.get_value(i, 'Close'))  \n",
    "        TR_l.append(TR)  \n",
    "        BP = df.get_value(i + 1, 'Close') - min(df.get_value(i + 1, 'Low'), df.get_value(i, 'Close'))  \n",
    "        BP_l.append(BP)  \n",
    "        i = i + 1  \n",
    "    UltO = pd.Series((4 * pd.rolling_sum(pd.Series(BP_l), 7) / pd.rolling_sum(pd.Series(TR_l), 7)) + (2 * pd.rolling_sum(pd.Series(BP_l), 14) / pd.rolling_sum(pd.Series(TR_l), 14)) + (pd.rolling_sum(pd.Series(BP_l), 28) / pd.rolling_sum(pd.Series(TR_l), 28)), name = 'Ultimate_Osc')  \n",
    "    df = df.join(UltO)  \n",
    "    return df\n",
    "\n",
    "#Donchian Channel  \n",
    "def DONCH(df, n):  \n",
    "    i = 0  \n",
    "    DC_l = []  \n",
    "    while i < n - 1:  \n",
    "        DC_l.append(0)  \n",
    "        i = i + 1  \n",
    "    i = 0  \n",
    "    while i + n - 1 < df.index[-1]:  \n",
    "        DC = max(df['High'].ix[i:i + n - 1]) - min(df['Low'].ix[i:i + n - 1])  \n",
    "        DC_l.append(DC)  \n",
    "        i = i + 1  \n",
    "    DonCh = pd.Series(DC_l, name = 'Donchian_' + str(n))  \n",
    "    DonCh = DonCh.shift(n - 1)  \n",
    "    df = df.join(DonCh)  \n",
    "    return df\n",
    "`\n",
    "#Standard Deviation  \n",
    "def STDDEV(df, n):  \n",
    "    df = df.join(pd.Series(pd.rolling_std(df['Close'], n), name = 'STD_' + str(n)))  \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indicators as shown by Peter Bakker at:\n",
    "https://www.quantopian.com/posts/technical-analysis-indicators-without-talib-code\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "25-Mar-2018: Fixed syntax to support the newest version of Pandas. Warnings should no longer appear.\n",
    "             Fixed some bugs regarding min_periods and NaN.\n",
    "\t\t\t If you find any bugs, please report to github.com/palmbook\n",
    "\"\"\"\n",
    "\n",
    "# Import Built-Ins\n",
    "import logging\n",
    "\n",
    "# Import Third-Party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Homebrew\n",
    "\n",
    "# Init Logging Facilities\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def moving_average(df, n):\n",
    "    \"\"\"Calculate the moving average for the given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    MA = pd.Series(df['Close'].rolling(n, min_periods=n).mean(), name='MA_' + str(n))\n",
    "    df = df.join(MA)\n",
    "    return df\n",
    "\n",
    "\n",
    "def exponential_moving_average(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    EMA = pd.Series(df['Close'].ewm(span=n, min_periods=n).mean(), name='EMA_' + str(n))\n",
    "    df = df.join(EMA)\n",
    "    return df\n",
    "\n",
    "\n",
    "def momentum(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame \n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = pd.Series(df['Close'].diff(n), name='Momentum_' + str(n))\n",
    "    df = df.join(M)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rate_of_change(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = df['Close'].diff(n - 1)\n",
    "    N = df['Close'].shift(n - 1)\n",
    "    ROC = pd.Series(M / N, name='ROC_' + str(n))\n",
    "    df = df.join(ROC)\n",
    "    return df\n",
    "\n",
    "\n",
    "def average_true_range(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    while i < df.index[-1]:\n",
    "        TR = max(df.loc[i + 1, 'High'], df.loc[i, 'Close']) - min(df.loc[i + 1, 'Low'], df.loc[i, 'Close'])\n",
    "        TR_l.append(TR)\n",
    "        i = i + 1\n",
    "    TR_s = pd.Series(TR_l)\n",
    "    ATR = pd.Series(TR_s.ewm(span=n, min_periods=n).mean(), name='ATR_' + str(n))\n",
    "    df = df.join(ATR)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bollinger_bands(df, n):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    MA = pd.Series(df['Close'].rolling(n, min_periods=n).mean())\n",
    "    MSD = pd.Series(df['Close'].rolling(n, min_periods=n).std())\n",
    "    b1 = 4 * MSD / MA\n",
    "    B1 = pd.Series(b1, name='BollingerB_' + str(n))\n",
    "    df = df.join(B1)\n",
    "    b2 = (df['Close'] - MA + 2 * MSD) / (4 * MSD)\n",
    "    B2 = pd.Series(b2, name='Bollinger%b_' + str(n))\n",
    "    df = df.join(B2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ppsr(df):\n",
    "    \"\"\"Calculate Pivot Points, Supports and Resistances for given data\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    PP = pd.Series((df['High'] + df['Low'] + df['Close']) / 3)\n",
    "    R1 = pd.Series(2 * PP - df['Low'])\n",
    "    S1 = pd.Series(2 * PP - df['High'])\n",
    "    R2 = pd.Series(PP + df['High'] - df['Low'])\n",
    "    S2 = pd.Series(PP - df['High'] + df['Low'])\n",
    "    R3 = pd.Series(df['High'] + 2 * (PP - df['Low']))\n",
    "    S3 = pd.Series(df['Low'] - 2 * (df['High'] - PP))\n",
    "    psr = {'PP': PP, 'R1': R1, 'S1': S1, 'R2': R2, 'S2': S2, 'R3': R3, 'S3': S3}\n",
    "    PSR = pd.DataFrame(psr)\n",
    "    df = df.join(PSR)\n",
    "    return df\n",
    "\n",
    "\n",
    "def stochastic_oscillator_k(df):\n",
    "    \"\"\"Calculate stochastic oscillator %K for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name='SO%k')\n",
    "    df = df.join(SOk)\n",
    "    return df\n",
    "\n",
    "\n",
    "def stochastic_oscillator_d(df, n):\n",
    "    \"\"\"Calculate stochastic oscillator %D for given data.\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name='SO%k')\n",
    "    SOd = pd.Series(SOk.ewm(span=n, min_periods=n).mean(), name='SO%d_' + str(n))\n",
    "    df = df.join(SOd)\n",
    "    return df\n",
    "\n",
    "\n",
    "def trix(df, n):\n",
    "    \"\"\"Calculate TRIX for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    EX1 = df['Close'].ewm(span=n, min_periods=n).mean()\n",
    "    EX2 = EX1.ewm(span=n, min_periods=n).mean()\n",
    "    EX3 = EX2.ewm(span=n, min_periods=n).mean()\n",
    "    i = 0\n",
    "    ROC_l = [np.nan]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        ROC = (EX3[i + 1] - EX3[i]) / EX3[i]\n",
    "        ROC_l.append(ROC)\n",
    "        i = i + 1\n",
    "    Trix = pd.Series(ROC_l, name='Trix_' + str(n))\n",
    "    df = df.join(Trix)\n",
    "    return df\n",
    "\n",
    "\n",
    "def average_directional_movement_index(df, n, n_ADX):\n",
    "    \"\"\"Calculate the Average Directional Movement Index for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :param n_ADX: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = []\n",
    "    DoI = []\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, 'High'] - df.loc[i, 'High']\n",
    "        DoMove = df.loc[i, 'Low'] - df.loc[i + 1, 'Low']\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    while i < df.index[-1]:\n",
    "        TR = max(df.loc[i + 1, 'High'], df.loc[i, 'Close']) - min(df.loc[i + 1, 'Low'], df.loc[i, 'Close'])\n",
    "        TR_l.append(TR)\n",
    "        i = i + 1\n",
    "    TR_s = pd.Series(TR_l)\n",
    "    ATR = pd.Series(TR_s.ewm(span=n, min_periods=n).mean())\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean() / ATR)\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean() / ATR)\n",
    "    ADX = pd.Series((abs(PosDI - NegDI) / (PosDI + NegDI)).ewm(span=n_ADX, min_periods=n_ADX).mean(),\n",
    "                    name='ADX_' + str(n) + '_' + str(n_ADX))\n",
    "    df = df.join(ADX)\n",
    "    return df\n",
    "\n",
    "\n",
    "def macd(df, n_fast, n_slow):\n",
    "    \"\"\"Calculate MACD, MACD Signal and MACD difference\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n_fast: \n",
    "    :param n_slow: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    EMAfast = pd.Series(df['Close'].ewm(span=n_fast, min_periods=n_slow).mean())\n",
    "    EMAslow = pd.Series(df['Close'].ewm(span=n_slow, min_periods=n_slow).mean())\n",
    "    MACD = pd.Series(EMAfast - EMAslow, name='MACD_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDsign = pd.Series(MACD.ewm(span=9, min_periods=9).mean(), name='MACDsign_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDdiff = pd.Series(MACD - MACDsign, name='MACDdiff_' + str(n_fast) + '_' + str(n_slow))\n",
    "    df = df.join(MACD)\n",
    "    df = df.join(MACDsign)\n",
    "    df = df.join(MACDdiff)\n",
    "    return df\n",
    "\n",
    "\n",
    "def mass_index(df):\n",
    "    \"\"\"Calculate the Mass Index for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    Range = df['High'] - df['Low']\n",
    "    EX1 = Range.ewm(span=9, min_periods=9).mean()\n",
    "    EX2 = EX1.ewm(span=9, min_periods=9).mean()\n",
    "    Mass = EX1 / EX2\n",
    "    MassI = pd.Series(Mass.rolling(25).sum(), name='Mass Index')\n",
    "    df = df.join(MassI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vortex_indicator(df, n):\n",
    "    \"\"\"Calculate the Vortex Indicator for given data.\n",
    "    \n",
    "    Vortex Indicator described here:\n",
    "        http://www.vortexindicator.com/VFX_VORTEX.PDF\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    TR = [0]\n",
    "    while i < df.index[-1]:\n",
    "        Range = max(df.loc[i + 1, 'High'], df.loc[i, 'Close']) - min(df.loc[i + 1, 'Low'], df.loc[i, 'Close'])\n",
    "        TR.append(Range)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    VM = [0]\n",
    "    while i < df.index[-1]:\n",
    "        Range = abs(df.loc[i + 1, 'High'] - df.loc[i, 'Low']) - abs(df.loc[i + 1, 'Low'] - df.loc[i, 'High'])\n",
    "        VM.append(Range)\n",
    "        i = i + 1\n",
    "    VI = pd.Series(pd.Series(VM).rolling(n).sum() / pd.Series(TR).rolling(n).sum(), name='Vortex_' + str(n))\n",
    "    df = df.join(VI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def kst_oscillator(df, r1, r2, r3, r4, n1, n2, n3, n4):\n",
    "    \"\"\"Calculate KST Oscillator for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param r1: \n",
    "    :param r2: \n",
    "    :param r3: \n",
    "    :param r4: \n",
    "    :param n1: \n",
    "    :param n2: \n",
    "    :param n3: \n",
    "    :param n4: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = df['Close'].diff(r1 - 1)\n",
    "    N = df['Close'].shift(r1 - 1)\n",
    "    ROC1 = M / N\n",
    "    M = df['Close'].diff(r2 - 1)\n",
    "    N = df['Close'].shift(r2 - 1)\n",
    "    ROC2 = M / N\n",
    "    M = df['Close'].diff(r3 - 1)\n",
    "    N = df['Close'].shift(r3 - 1)\n",
    "    ROC3 = M / N\n",
    "    M = df['Close'].diff(r4 - 1)\n",
    "    N = df['Close'].shift(r4 - 1)\n",
    "    ROC4 = M / N\n",
    "    KST = pd.Series(\n",
    "        ROC1.rolling(n1).sum() + ROC2.rolling(n2).sum() * 2 + ROC3.rolling(n3).sum() * 3 + ROC4.rolling(n4).sum() * 4,\n",
    "        name='KST_' + str(r1) + '_' + str(r2) + '_' + str(r3) + '_' + str(r4) + '_' + str(n1) + '_' + str(\n",
    "            n2) + '_' + str(n3) + '_' + str(n4))\n",
    "    df = df.join(KST)\n",
    "    return df\n",
    "\n",
    "\n",
    "def relative_strength_index(df, n):\n",
    "    \"\"\"Calculate Relative Strength Index(RSI) for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, 'High'] - df.loc[i, 'High']\n",
    "        DoMove = df.loc[i, 'Low'] - df.loc[i + 1, 'Low']\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(PosDI / (PosDI + NegDI), name='RSI_' + str(n))\n",
    "    df = df.join(RSI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def true_strength_index(df, r, s):\n",
    "    \"\"\"Calculate True Strength Index (TSI) for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param r: \n",
    "    :param s: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = pd.Series(df['Close'].diff(1))\n",
    "    aM = abs(M)\n",
    "    EMA1 = pd.Series(M.ewm(span=r, min_periods=r).mean())\n",
    "    aEMA1 = pd.Series(aM.ewm(span=r, min_periods=r).mean())\n",
    "    EMA2 = pd.Series(EMA1.ewm(span=s, min_periods=s).mean())\n",
    "    aEMA2 = pd.Series(aEMA1.ewm(span=s, min_periods=s).mean())\n",
    "    TSI = pd.Series(EMA2 / aEMA2, name='TSI_' + str(r) + '_' + str(s))\n",
    "    df = df.join(TSI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def accumulation_distribution(df, n):\n",
    "    \"\"\"Calculate Accumulation/Distribution for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']\n",
    "    M = ad.diff(n - 1)\n",
    "    N = ad.shift(n - 1)\n",
    "    ROC = M / N\n",
    "    AD = pd.Series(ROC, name='Acc/Dist_ROC_' + str(n))\n",
    "    df = df.join(AD)\n",
    "    return df\n",
    "\n",
    "\n",
    "def chaikin_oscillator(df):\n",
    "    \"\"\"Calculate Chaikin Oscillator for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']\n",
    "    Chaikin = pd.Series(ad.ewm(span=3, min_periods=3).mean() - ad.ewm(span=10, min_periods=10).mean(), name='Chaikin')\n",
    "    df = df.join(Chaikin)\n",
    "    return df\n",
    "\n",
    "\n",
    "def money_flow_index(df, n):\n",
    "    \"\"\"Calculate Money Flow Index and Ratio for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    i = 0\n",
    "    PosMF = [0]\n",
    "    while i < df.index[-1]:\n",
    "        if PP[i + 1] > PP[i]:\n",
    "            PosMF.append(PP[i + 1] * df.loc[i + 1, 'Volume'])\n",
    "        else:\n",
    "            PosMF.append(0)\n",
    "        i = i + 1\n",
    "    PosMF = pd.Series(PosMF)\n",
    "    TotMF = PP * df['Volume']\n",
    "    MFR = pd.Series(PosMF / TotMF)\n",
    "    MFI = pd.Series(MFR.rolling(n, min_periods=n).mean(), name='MFI_' + str(n))\n",
    "    df = df.join(MFI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def on_balance_volume(df, n):\n",
    "    \"\"\"Calculate On-Balance Volume for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    OBV = [0]\n",
    "    while i < df.index[-1]:\n",
    "        if df.loc[i + 1, 'Close'] - df.loc[i, 'Close'] > 0:\n",
    "            OBV.append(df.loc[i + 1, 'Volume'])\n",
    "        if df.loc[i + 1, 'Close'] - df.loc[i, 'Close'] == 0:\n",
    "            OBV.append(0)\n",
    "        if df.loc[i + 1, 'Close'] - df.loc[i, 'Close'] < 0:\n",
    "            OBV.append(-df.loc[i + 1, 'Volume'])\n",
    "        i = i + 1\n",
    "    OBV = pd.Series(OBV)\n",
    "    OBV_ma = pd.Series(OBV.rolling(n, min_periods=n).mean(), name='OBV_' + str(n))\n",
    "    df = df.join(OBV_ma)\n",
    "    return df\n",
    "\n",
    "\n",
    "def force_index(df, n):\n",
    "    \"\"\"Calculate Force Index for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    F = pd.Series(df['Close'].diff(n) * df['Volume'].diff(n), name='Force_' + str(n))\n",
    "    df = df.join(F)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ease_of_movement(df, n):\n",
    "    \"\"\"Calculate Ease of Movement for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    EoM = (df['High'].diff(1) + df['Low'].diff(1)) * (df['High'] - df['Low']) / (2 * df['Volume'])\n",
    "    Eom_ma = pd.Series(EoM.rolling(n, min_periods=n).mean(), name='EoM_' + str(n))\n",
    "    df = df.join(Eom_ma)\n",
    "    return df\n",
    "\n",
    "\n",
    "def commodity_channel_index(df, n):\n",
    "    \"\"\"Calculate Commodity Channel Index for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    CCI = pd.Series((PP - PP.rolling(n, min_periods=n).mean()) / PP.rolling(n, min_periods=n).std(),\n",
    "                    name='CCI_' + str(n))\n",
    "    df = df.join(CCI)\n",
    "    return df\n",
    "\n",
    "\n",
    "def coppock_curve(df, n):\n",
    "    \"\"\"Calculate Coppock Curve for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    M = df['Close'].diff(int(n * 11 / 10) - 1)\n",
    "    N = df['Close'].shift(int(n * 11 / 10) - 1)\n",
    "    ROC1 = M / N\n",
    "    M = df['Close'].diff(int(n * 14 / 10) - 1)\n",
    "    N = df['Close'].shift(int(n * 14 / 10) - 1)\n",
    "    ROC2 = M / N\n",
    "    Copp = pd.Series((ROC1 + ROC2).ewm(span=n, min_periods=n).mean(), name='Copp_' + str(n))\n",
    "    df = df.join(Copp)\n",
    "    return df\n",
    "\n",
    "\n",
    "def keltner_channel(df, n):\n",
    "    \"\"\"Calculate Keltner Channel for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    KelChM = pd.Series(((df['High'] + df['Low'] + df['Close']) / 3).rolling(n, min_periods=n).mean(),\n",
    "                       name='KelChM_' + str(n))\n",
    "    KelChU = pd.Series(((4 * df['High'] - 2 * df['Low'] + df['Close']) / 3).rolling(n, min_periods=n).mean(),\n",
    "                       name='KelChU_' + str(n))\n",
    "    KelChD = pd.Series(((-2 * df['High'] + 4 * df['Low'] + df['Close']) / 3).rolling(n, min_periods=n).mean(),\n",
    "                       name='KelChD_' + str(n))\n",
    "    df = df.join(KelChM)\n",
    "    df = df.join(KelChU)\n",
    "    df = df.join(KelChD)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ultimate_oscillator(df):\n",
    "    \"\"\"Calculate Ultimate Oscillator for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    BP_l = [0]\n",
    "    while i < df.index[-1]:\n",
    "        TR = max(df.loc[i + 1, 'High'], df.loc[i, 'Close']) - min(df.loc[i + 1, 'Low'], df.loc[i, 'Close'])\n",
    "        TR_l.append(TR)\n",
    "        BP = df.loc[i + 1, 'Close'] - min(df.loc[i + 1, 'Low'], df.loc[i, 'Close'])\n",
    "        BP_l.append(BP)\n",
    "        i = i + 1\n",
    "    UltO = pd.Series((4 * pd.Series(BP_l).rolling(7).sum() / pd.Series(TR_l).rolling(7).sum()) + (\n",
    "                2 * pd.Series(BP_l).rolling(14).sum() / pd.Series(TR_l).rolling(14).sum()) + (\n",
    "                                 pd.Series(BP_l).rolling(28).sum() / pd.Series(TR_l).rolling(28).sum()),\n",
    "                     name='Ultimate_Osc')\n",
    "    df = df.join(UltO)\n",
    "    return df\n",
    "\n",
    "\n",
    "def donchian_channel(df, n):\n",
    "    \"\"\"Calculate donchian channel of given pandas data frame.\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n:\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    dc_l = []\n",
    "    while i < n - 1:\n",
    "        dc_l.append(0)\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    while i + n - 1 < df.index[-1]:\n",
    "        dc = max(df['High'].ix[i:i + n - 1]) - min(df['Low'].ix[i:i + n - 1])\n",
    "        dc_l.append(dc)\n",
    "        i += 1\n",
    "\n",
    "    donchian_chan = pd.Series(dc_l, name='Donchian_' + str(n))\n",
    "    donchian_chan = donchian_chan.shift(n - 1)\n",
    "    return df.join(donchian_chan)\n",
    "\n",
    "\n",
    "def standard_deviation(df, n):\n",
    "    \"\"\"Calculate Standard Deviation for given data.\n",
    "    \n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    df = df.join(pd.Series(df['Close'].rolling(n, min_periods=n).std(), name='STD_' + str(n)))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
